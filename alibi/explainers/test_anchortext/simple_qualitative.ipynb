{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robert/anaconda3/envs/alibi/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import string\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from alibi.explainers import AnchorText\n",
    "from alibi.datasets import fetch_movie_sentiment\n",
    "from alibi.utils.download import spacy_model\n",
    "from alibi.utils.lang_model import DistilbertBaseUncased, BertBaseUncased, RobertaBase\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load movie review dataset\n",
    "\n",
    "The `fetch_movie_sentiment` function returns a `Bunch` object containing the features, the targets and the target names for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = fetch_movie_sentiment()\n",
    "movies.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = movies.data\n",
    "labels = movies.target\n",
    "target_names = movies.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, train_labels, test_labels = train_test_split(data, labels, test_size=.2, random_state=42)\n",
    "train, val, train_labels, val_labels = train_test_split(train, train_labels, test_size=.1, random_state=42)\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "val_labels = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply CountVectorizer to training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=1)\n",
    "vectorizer.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "clf = LogisticRegression(solver='liblinear')\n",
    "clf.fit(vectorizer.transform(train), train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fn = lambda x: clf.predict(vectorizer.transform(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.9801624284382905\n",
      "Validation accuracy 0.7544910179640718\n",
      "Test accuracy 0.7589841878294202\n"
     ]
    }
   ],
   "source": [
    "preds_train = predict_fn(train)\n",
    "preds_val = predict_fn(val)\n",
    "preds_test = predict_fn(test)\n",
    "print('Train accuracy', accuracy_score(train_labels, preds_train))\n",
    "print('Validation accuracy', accuracy_score(val_labels, preds_val))\n",
    "print('Test accuracy', accuracy_score(test_labels, preds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load spaCy model\n",
    "\n",
    "English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Assigns word vectors, context-specific token vectors, POS tags, dependency parse and named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'en_core_web_md'\n",
    "spacy_model(model=model)\n",
    "nlp = spacy.load(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"DistilbertBaseUncased\": DistilbertBaseUncased(),\n",
    "    \"BertBaseUncased\":  BertBaseUncased(),\n",
    "    \"RobertaBase\": RobertaBase(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"This is a good movie.\",\n",
    "    \"The movie was excellent.\",\n",
    "    \"The best story ever.\",\n",
    "    \"Worst movie ever.\",\n",
    "    \"Everyone played terrible.\",\n",
    "    \"The book was horrible.\",\n",
    "]\n",
    "\n",
    "tests = {\n",
    "    \"model_name\": [\"DistilbertBaseUncased\", \"BertBaseUncased\", \"RobertaBase\"],\n",
    "    \"filling_method\": [\"parallel\"],          # ['parallel', 'autoregressive'],\n",
    "    \"sample_proba\": [1.0, 0.5],\n",
    "    \"mask_templates\": [0.1, 0.2, 0.5],\n",
    "    \"temperature\": [0.1, 1.0, 10., 100.],\n",
    "    \"punctuation\": [\"\", string.punctuation],\n",
    "    \"stopwords\": [[], [\"in\", \"a\", \"the\", \"this\", \"those\", \"but\"]]\n",
    "}\n",
    "\n",
    "class_names = movies.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_explanation(explanation, config: dict = dict()) -> str:\n",
    "    s = config['model_name'] + '\\n=======================\\n'\n",
    "    s += 'Confing: ' + str(config) + '\\n\\n'\n",
    "    s += 'Text: ' +  config['text'] + '\\n\\n'\n",
    "    \n",
    "    if len(explanation.anchor) == 0:\n",
    "        s += 'WARNING !!! EMPTY ANCHOR!\\n'\n",
    "    \n",
    "    s += 'Anchor: %s\\n' % (' AND '.join(explanation.anchor))\n",
    "    s += 'Precision: %.2f\\n' % explanation.precision\n",
    "    \n",
    "    # print examples covered as True\n",
    "    s += '\\n\\nExamples where anchor applies and model predicts %s:\\n' % config['pred']\n",
    "    if len(explanation.raw['examples']):\n",
    "        s += '\\n'.join([x for x in explanation.raw['examples'][-1]['covered_true']])\n",
    "    \n",
    "    # print examples covered as False\n",
    "    s += '\\n\\nExamples where anchor applies and model predicts %s:\\n' % config['alternative']\n",
    "    if len(explanation.raw['examples']):\n",
    "        s += '\\n'.join([x for x in explanation.raw['examples'][-1]['covered_false']])\n",
    "    \n",
    "    s += '\\n\\n\\n'\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "288it [33:00,  6.88s/it]\n",
      "272it [28:13, 17.37s/it]"
     ]
    }
   ],
   "source": [
    "output_file = \"qualitative_tests.txt\"\n",
    "\n",
    "for text in texts:\n",
    "    # compute text prediction\n",
    "    pred = class_names[predict_fn([text])[0]]\n",
    "    alternative = class_names[1 - predict_fn([text])[0]]\n",
    "    \n",
    "    # get all combinations\n",
    "    values = tests.values()\n",
    "    combinations = itertools.product(*values)\n",
    "    \n",
    "    for comb in tqdm(combinations):\n",
    "        config = dict(zip(tests.keys(), comb))\n",
    "        config['pred'] = pred\n",
    "        config['alternative'] = alternative\n",
    "        config['text'] = text\n",
    "\n",
    "        # define explainer\n",
    "        np.random.seed(0)\n",
    "        explainer = AnchorText(language_model=models[config['model_name']], predictor=predict_fn)\n",
    "\n",
    "        # compute explanation\n",
    "        explanation = explainer.explain(\n",
    "            text,\n",
    "            threshold=0.95,\n",
    "            sampling_method=\"language_model\",\n",
    "            top_n=20,\n",
    "            filling_method=config['filling_method'],           # this can vary\n",
    "            sample_proba=config['sample_proba'],               # this can vary\n",
    "            mask_templates=config['mask_templates'],           # this can vary\n",
    "            stopwords=config['stopwords'],                     # this can vary\n",
    "            punctuation=config['punctuation']                  # this can vary\n",
    "        )\n",
    "\n",
    "        # convert expalanation to string\n",
    "        exp = build_explanation(explanation, config=config)\n",
    "        \n",
    "        # append it to output file\n",
    "        with open(output_file, \"a+\") as fout:\n",
    "            fout.write(exp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alibi",
   "language": "python",
   "name": "alibi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
