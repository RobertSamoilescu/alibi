{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robert/anaconda3/envs/alibi/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import string\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from alibi.explainers import AnchorText\n",
    "from alibi.datasets import fetch_movie_sentiment\n",
    "from alibi.utils.download import spacy_model\n",
    "from alibi.utils.lang_model import DistilbertBaseUncased, BertBaseUncased, RobertaBase\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load movie review dataset\n",
    "\n",
    "The `fetch_movie_sentiment` function returns a `Bunch` object containing the features, the targets and the target names for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = fetch_movie_sentiment()\n",
    "movies.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = movies.data\n",
    "labels = movies.target\n",
    "target_names = movies.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, train_labels, test_labels = train_test_split(data, labels, test_size=.2, random_state=42)\n",
    "train, val, train_labels, val_labels = train_test_split(train, train_labels, test_size=.1, random_state=42)\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "val_labels = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply CountVectorizer to training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=1)\n",
    "vectorizer.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "clf = LogisticRegression(solver='liblinear')\n",
    "clf.fit(vectorizer.transform(train), train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fn = lambda x: clf.predict(vectorizer.transform(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.9801624284382905\n",
      "Validation accuracy 0.7544910179640718\n",
      "Test accuracy 0.7589841878294202\n"
     ]
    }
   ],
   "source": [
    "preds_train = predict_fn(train)\n",
    "preds_val = predict_fn(val)\n",
    "preds_test = predict_fn(test)\n",
    "print('Train accuracy', accuracy_score(train_labels, preds_train))\n",
    "print('Validation accuracy', accuracy_score(val_labels, preds_val))\n",
    "print('Test accuracy', accuracy_score(test_labels, preds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load spaCy model\n",
    "\n",
    "English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Assigns word vectors, context-specific token vectors, POS tags, dependency parse and named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'en_core_web_md'\n",
    "spacy_model(model=model)\n",
    "nlp = spacy.load(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_model = DistilbertBaseUncased()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 299  379  997 1601  264  229 1774 1968  458  252]\n"
     ]
    }
   ],
   "source": [
    "indices = np.random.choice(len(test), size=10, replace=False)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = AnchorText(nlp=nlp, language_model=lang_model, predictor=predict_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_explanation(explanation) -> str:\n",
    "    s = ''\n",
    "    s += 'Anchor: %s\\n' % (' AND '.join(explanation.anchor))\n",
    "    s += 'Precision: %.2f\\n' % explanation.precision\n",
    "    \n",
    "    # print examples covered as True\n",
    "    s += '\\n\\nExamples where anchor applies and model predicts %s:\\n' % pred\n",
    "    if len(explanation.raw['examples']):\n",
    "        s += '\\n'.join([x for x in explanation.raw['examples'][-1]['covered_true']])\n",
    "    \n",
    "    # print examples covered as False\n",
    "    s += '\\n\\nExamples where anchor applies and model predicts %s:\\n' % alternative\n",
    "    if len(explanation.raw['examples']):\n",
    "        s += '\\n'.join([x for x in explanation.raw['examples'][-1]['covered_false']])\n",
    "    \n",
    "    s += '\\n\\n\\n'\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"threshold\": 0.95,\n",
    "    \"top_n\": 50,\n",
    "    \"sample_proba\": 0.5,\n",
    "    \"filling_method\": 'parallel',\n",
    "    \"prec_mask_templates\": 0.1,\n",
    "    \"punctuation\": string.punctuation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = movies.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = test[indices[0]]\n",
    "\n",
    "# compute text prediction\n",
    "pred = class_names[predict_fn([text])[0]]\n",
    "alternative = class_names[1 - predict_fn([text])[0]]\n",
    "\n",
    "# similarity explanation\n",
    "sim_explanation = explainer.explain(\n",
    "    text,\n",
    "    sampling_method=\"similarity\",\n",
    "    **config\n",
    ")\n",
    "\n",
    "# language model explanation\n",
    "lm_explanation = explainer.explain(\n",
    "    text,\n",
    "    sampling_method='language_model',\n",
    "    **config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: a fantastically vital movie that manages to invest real humor , sensuality , and sympathy into a story about two adolescent boys .\n"
     ]
    }
   ],
   "source": [
    "print(\"Original text:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: manages AND real AND that\n",
      "Precision: 0.97\n",
      "\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "each elegantly vital movie that manages to invest real anthology , sensuality , and compassion to this story well two adolescent juniors .\n",
      "a fantastically necessitate filmmaker that manages to invest real anthology , sensuality , and outrage into a epilogue about two maternal teenagers .\n",
      "a fantastically vital porno that manages to consider real humor , sensuality , and heartache into this backstory almost two adolescent daddies .\n",
      "a fantastically satisfactory movie that manages to invest real impersonation , longing , and heartbreak into some heroine about two adolescent boys .\n",
      "any fantastically vital movie that manages to realize real humor , sensuality , and defiance inside any story obviously two adoptive swimmers .\n",
      "a surprisingly crucial foto that manages to privatize real humor , sensuality , and discontent like an protagonist only two adolescent boys .\n",
      "a disturbingly effective soundtrack that manages to privatize real humor , sensuality , and sympathy into some plot about two adolescent boys .\n",
      "this fantastically vital pic that manages to sustain real spoof , sensuality , and discord into a character much two adolescent boys .\n",
      "a pleasantly considerable requiem that manages to undertake real humor , sensuality , and sympathy into an epilogue about two uncensored men .\n",
      "some fantastically vital cartoon that manages to invest real humor , sensuality , and despair into a story kind two nude boys .\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "a fantastically vital movie that manages to succeed real nonfiction , impure , and unhappiness onto the writer nearly two sexy olds .\n",
      "another unbelievably particular movie that manages to borrow real humor , thirst , and sympathy into some prologue sort two adolescent boys .\n",
      "a perfectly vital movie that manages to invest real nonfiction , sensuality , and apology off a fable there two inebriated adults .\n",
      "another fantastically vital movie that manages to invest real professionalism , lament , and outcry off this story about two teenaged gents .\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display similarity explanation\n",
    "print(build_explanation(sim_explanation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: manages\n",
      "Precision: 0.98\n",
      "\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "a fairy adult comic and manages to invest in humor, sadness, and sympathy in a story about two adolescent boys.\n",
      "a novel novel strip who manages an invest in humor, wit, and sympathy in a story about two adolescent boys.\n",
      "a teenage writer plot quartet manages to invest in humor, irony, and sympathy in a story about two adolescent boys.\n",
      "a fictional and strip quartet manages to invest in humor, courage, and sympathy into a story about two adolescent boys.\n",
      "a very drama fiction who manages to invest in humor, humor, and sympathy on a story about two adolescent boys.\n",
      "a mysterious comic tale man manages to invest between humor, humor, and sympathy during a story about two adolescent boys.\n",
      "a little suspense strip that manages to invest in humor, imagination, and sympathy in a story about two adolescent boys.\n",
      "a teenage humor whose that manages to invest in humor, romance, and sympathy on a story about two adolescent boys.\n",
      "a traditional humor teen man manages to invest in humor, warmth, and sympathy in a story about two adolescent boys.\n",
      "a witty suspense romance that manages to invest in humor, irony, and sympathy in a story about two adolescent boys.\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "next a vital movie that manages to invest real depth, sensuality, and creativity into a tale about two erotic players.\n",
      "a less vital movie channel manages the in between banks, frank, and friends through a documentary about two homeless characters.\n",
      "a potentially vital movie archive manages six by between actors, sam, and charlie contains a movie about two thousand babies.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display language model explanation\n",
    "print(build_explanation(lm_explanation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "[W008] Evaluating Lexeme.similarity based on empty vectors.\n"
     ]
    }
   ],
   "source": [
    "text = test[indices[1]]\n",
    "\n",
    "# compute text prediction\n",
    "pred = class_names[predict_fn([text])[0]]\n",
    "alternative = class_names[1 - predict_fn([text])[0]]\n",
    "\n",
    "# similarity explanation\n",
    "sim_explanation = explainer.explain(\n",
    "    text,\n",
    "    sampling_method=\"similarity\",\n",
    "    **config\n",
    ")\n",
    "\n",
    "# language model explanation\n",
    "lm_explanation = explainer.explain(\n",
    "    text,\n",
    "    sampling_method='language_model',\n",
    "    **config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: there is a refreshing absence of cynicism in stuart little 2--quite a rarity , even in the family film market . eventually , it wins you over .\n"
     ]
    }
   ],
   "source": [
    "print(\"Original text:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: refreshing AND family AND rarity AND film AND a AND of AND even\n",
      "Precision: 0.96\n",
      "\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "there is another refreshing slowness of animus in swift difficult 2 - -quite a rarity , even through every family film trend . immediatly , it wins you over .\n",
      "there is each refreshing certainty of immaturity in british sweet 2 - -quite a rarity , even in the family film consumer . again , it wins you over .\n",
      "there is a refreshing unwillingness of cynicism like swift wonderful 2 - video a rarity , even towards those family film business . slowly , it wins you over .\n",
      "there is a refreshing inconsistency of apathy in scottish little 2 - friend a rarity , even if the family film foundry . still , it sweeps you over .\n",
      "there is a refreshing likelyhood of ignorance in stuart little 2 - -quite a rarity , even whether a family film equity . underway , it wins you over .\n",
      "there is a refreshing absence of animus in stuart little 2 - -quite a rarity , even in the family film market . finally , it jumps you over .\n",
      "there is another refreshing exclusion of immorality behind stuart little 2 - something a rarity , even to some family film market . swiftly , it wins you up .\n",
      "there is a refreshing absence of cynicism than stuart little 2 - -quite a rarity , even in both family film investment . thus , it wins you over .\n",
      "there is a refreshing possibility of cynicism with australian little 2 - day a rarity , even along the family film market . eventually , it draws you over .\n",
      "there is any refreshing absence of cynicism in scottish itty 2 - week a rarity , even in these family film market . eventually , it deserves you up .\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "there is no refreshing consequence of cynicism in scottish little 2 - everything a rarity , even amidst both family film pricing . assuredly , it wins you over .\n",
      "there is another refreshing reluctance of cynicism throughout stuart little 2 - -quite a rarity , even beyond all family film market . probably , it wins you over .\n",
      "there is no refreshing absence of cynicism under stuart few 2 - everyone a rarity , even in the family film product . soon , it chances you up .\n",
      "there is the refreshing determination of cynicism in scottish little 2 - -quite a rarity , even in another family film market . already , it wins you over .\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display similarity explanation\n",
    "print(build_explanation(sim_explanation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: refreshing AND family AND you AND it AND film AND rarity\n",
      "Precision: 0.97\n",
      "\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "there is a refreshing sense of cynicism in bbc babylon 2 - - quite a rarity, especially in the family film market. unfortunately, it gets you over.\n",
      "there is a refreshing dose of cynicism in bad film 2 - - quite a rarity, especially in the family film market. however, it makes you over.\n",
      "there is a refreshing sense of cynicism in paddington region 2 - - quite a rarity, especially in pakistani family film market. oh, it turns you over.\n",
      "there is a refreshing bit of cynicism in happy movie 2 - - quite a rarity, particularly in telugu family film market. naturally, it gets you over.\n",
      "there is a refreshing atmosphere of cynicism in bad picture 2 - - quite a rarity, especially in the family film market. hopefully, it wins you over.\n",
      "there is a refreshing streak of cynicism in bbc playstation 2 - - quite a rarity, especially in russian family film market. sadly, it wins you over.\n",
      "there is a refreshing sense of cynicism in paddington type 2 - - quite a rarity, especially in the family film market. well, it takes you over.\n",
      "there is a refreshing sense of cynicism in psycho robot 2 - - quite a rarity, particularly in bollywood family film market. anyway, it won you over.\n",
      "there is a refreshing sense of cynicism in ps motel 2 - - quite a rarity, especially in american family film market. now, it turns you over.\n",
      "there is a refreshing sense of cynicism in jaws episode 2 - - quite a rarity, especially in russian family film market. otherwise, it turns you over.\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "it is a refreshing little entertainment introduced in category pictures 2 - - hardly a rarity, and for the family film series. finally, it wins you over.\n",
      "it is a refreshing cartoon feature ending in rule movie 2 - - probably a rarity, so than the family film genre. sadly, it wins you over.\n",
      "miss ’ presents refreshing with good features in stuart little guy - - quite relatively rarity, particularly in the family film market. eventually, it wins you over.\n",
      "im is features refreshing “ star ” in stuart little falls - - quite a rarity, especially in dutch family film market. eventually, it wins you over.\n",
      "doug the features refreshing a as — in stuart little star - - quite near rarity, especially in british family film market. eventually, it wins you over.\n",
      "jack “ ’ refreshing features was starring in stuart little film - - quite its rarity, particularly in the family film market. eventually, it wins you over.\n",
      "this is a refreshing example of cynicism as teen rated 2 - - quite a rarity, especially in taiwanese family film market. unfortunately, it wins you over.\n",
      "there is a refreshing absence american cynicism from stuart little feat - - not finds rarity, especially in horror family film series. unfortunately, it wins you out.\n",
      "that is a refreshing blockbuster movie made starring stuart mc scott - - almost a rarity, even in the family film market. ultimately, it wins you money.\n",
      "it is a refreshing movie between by comedian stuart van productions - - — a rarity, even in the family film market. sadly, it wins you more.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display language model explanation\n",
    "print(build_explanation(lm_explanation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find an result satisfying the 0.95 precision constraint. Now returning the best non-eligible result.\n"
     ]
    }
   ],
   "source": [
    "text = test[indices[2]]\n",
    "\n",
    "# compute text prediction\n",
    "pred = class_names[predict_fn([text])[0]]\n",
    "alternative = class_names[1 - predict_fn([text])[0]]\n",
    "\n",
    "# similarity explanation\n",
    "sim_explanation = explainer.explain(\n",
    "    text,\n",
    "    sampling_method=\"similarity\",\n",
    "    **config\n",
    ")\n",
    "\n",
    "# language model explanation\n",
    "lm_explanation = explainer.explain(\n",
    "    text,\n",
    "    sampling_method='language_model',\n",
    "    **config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: it doesn't offer audiences any way of gripping what its point is , or even its attitude toward its subject .\n"
     ]
    }
   ],
   "source": [
    "print(\"Original text:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: offer AND point AND audiences AND what AND toward AND its AND subject\n",
      "Precision: 0.63\n",
      "\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "it does n't offer audiences those sense of gripping what its point is , or possibly its attitude toward its subject .\n",
      "it does n't offer audiences some way under relaying what its point is , or possibly its mindset toward its subject .\n",
      "it does n't offer audiences any way between gripping what its point is , or even its discipline toward its subject .\n",
      "it does n't offer audiences some way for provoking what its point is , or possibly its zeal toward its subject .\n",
      "it does n't offer audiences a part midst gripping what its point is , or already its piety toward its subject .\n",
      "it does n't offer audiences these attention whether gripping what its point is , or sometimes its ethos toward its subject .\n",
      "it does n't offer audiences any way as gripping what its point is , or maybe its belief toward its subject .\n",
      "it does n't offer audiences any desire amidst gripping what its point is , or indeed its diligence toward its subject .\n",
      "it does n't offer audiences any situation than facilitating what its point is , or obviously its attitude toward its subject .\n",
      "it does n't offer audiences any everyone of relaying what its point is , or enough its attitude toward its subject .\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "it does n't offer audiences any way into gripping what its point is , or even its sense toward its subject .\n",
      "it does n't offer audiences both way of gripping what its point is , or even its attitude toward its subject .\n",
      "it does n't offer audiences both rest after bringing what its point is , or still its attitude toward its subject .\n",
      "it does n't offer audiences these future amid entertaining what its point is , or even its respect toward its subject .\n",
      "it does n't offer audiences both way of gripping what its point is , or just its attitude toward its subject .\n",
      "it does n't offer audiences these way of stimulating what its point is , or well its attitude toward its subject .\n",
      "it does n't offer audiences another way of gripping what its point is , or not its attitude toward its subject .\n",
      "it does n't offer audiences any longing towards gripping what its point is , or even its attitude toward its subject .\n",
      "it does n't offer audiences no way that bringing what its point is , or enough its attitude toward its subject .\n",
      "it does n't offer audiences these way of challenging what its point is , or there its rudeness toward its subject .\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display similarity explanation\n",
    "print(build_explanation(sim_explanation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: doesn AND point AND its AND offer AND audiences\n",
      "Precision: 0.97\n",
      "\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "it doesn't offer audiences any way of gripping what any point represents, or controlling its contents about its subject.\n",
      "it doesn't offer audiences any way of gripping what a point remains, or expanding its implications to its subject.\n",
      "it doesn't offer audiences any way of gripping what is point takes, or changing its presentation about its subject.\n",
      "it doesn't offer audiences any way of gripping what its point represents, or explaining its significance on its subject.\n",
      "it doesn't offer audiences any way of gripping what its point lacks, or telling its appeal into its subject.\n",
      "it doesn't offer audiences any way of gripping what viewers point involves, or turning its story around its subject.\n",
      "it doesn't offer audiences any way of gripping what the point represents, or questioning its audience behind its subject.\n",
      "it doesn't offer audiences any way of gripping what that point implied, or shifting its meaning about its subject.\n",
      "it doesn't offer audiences any way of determining what its point represents, or simply its attitudes toward its audience.\n",
      "it doesn't offer audiences any way of discussing what its point means, or merely further attitudes toward its audiences.\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "it doesn't offer audiences any way of gripping what its point is, or bending its audience about its subject.\n",
      "it doesn't offer audiences any way of gripping what it point holds, or adjusting its perspective on its subject.\n",
      "it doesn't offer audiences such about deeply gripping what its point is, its and sympathetic attitude to its subject.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display language model explanation\n",
    "print(build_explanation(lm_explanation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = test[indices[3]]\n",
    "\n",
    "# compute text prediction\n",
    "pred = class_names[predict_fn([text])[0]]\n",
    "alternative = class_names[1 - predict_fn([text])[0]]\n",
    "\n",
    "# similarity explanation\n",
    "sim_explanation = explainer.explain(\n",
    "    text,\n",
    "    sampling_method=\"similarity\",\n",
    "    **config\n",
    ")\n",
    "\n",
    "# language model explanation\n",
    "lm_explanation = explainer.explain(\n",
    "    text,\n",
    "    sampling_method='language_model',\n",
    "    **config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: rodriguez . . . was unable to reproduce the special spark between the characters that made the first film such a delight .\n"
     ]
    }
   ],
   "source": [
    "print(\"Original text:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: unable AND rodriguez AND characters\n",
      "Precision: 0.95\n",
      "\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "rodriguez . . . was unable to transmit the perfect spark within the characters which made the exciting film such a bliss .\n",
      "rodriguez . . . was unable to reproduce this special fire between the characters that made both important film such the delight .\n",
      "rodriguez . . . was unable to exploit some available extinguisher beyond the characters that made the own footage such a delight .\n",
      "rodriguez . . . was unable to alter another special spark with all characters that came the original poster such an relish .\n",
      "rodriguez . . . was unable to reproduce the regular ignition between any characters whatever made these previous fable such a delight .\n",
      "rodriguez . . . was unable to alter any special spark between some characters that made some other film such a heaven .\n",
      "rodriguez . . . was unable to reproduce an special provocation between any characters whatever convinced the first filmmaker such a delight .\n",
      "rodriguez . . . was unable to undertake the special silencer between the characters that made the first film such some delight .\n",
      "rodriguez . . . was unable to visualize both such spark between another characters whatever sent a able film such an delight .\n",
      "rodriguez . . . was unable to produce any special spark versus all characters whatever carried the first film such a delight .\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "rodriguez . . . was unable to evoke the extraordinary spark while the characters that made a first film such a delight .\n",
      "rodriguez . . . was unable to reproduce the unmatched originality arround a characters that used some first poster such a delight .\n",
      "rodriguez . . . was unable to reproduce both rare surge above the characters whatever made the subsequent film such both delight .\n",
      "rodriguez . . . was unable to reproduce each special sense although the characters that mentioned those first film such a delight .\n",
      "rodriguez . . . was unable to differentiation the certain radiant as each characters which helped the first film such a excitement .\n",
      "rodriguez . . . was unable to derive another wonderful excitement above the characters that became the original film such a delight .\n",
      "rodriguez . . . was unable to infer the unique spark between each characters whatever made the first documentary such a delight .\n",
      "rodriguez . . . was unable to reproduce the unique desire throughout another characters which made the first remake such a delight .\n",
      "rodriguez . . . was unable to morph the wonderful spark although the characters that made the first movie such no delight .\n",
      "rodriguez . . . was unable to reproduce every necessary byproduct because these characters that made the first adaptation such a delight .\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display similarity explanation\n",
    "print(build_explanation(sim_explanation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: spark AND rodriguez AND unable AND made AND the\n",
      "Precision: 0.97\n",
      "\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "rodriguez... definitely unable to contain the emotional spark through the characters that made the show become such as heroes.\n",
      "rodriguez... deliberately unable to satisfy the magic spark from the characters that made the audience have such things memorable.\n",
      "rodriguez... completely unable to remove the emotional spark from the characters that made the movie play such an events.\n",
      "rodriguez... but unable to capture the drama spark beyond the characters that made the story make such many successes.\n",
      "rodriguez... were unable to define the main spark or the characters that made the protagonist experience such frequent fail.\n",
      "rodriguez... is unable to find the drama spark from the characters that made the comic develop such powerful entertainment.\n",
      "rodriguez... purposely unable to produce the basic spark by the characters that made the villain play such passionate characters.\n",
      "rodriguez... apparently unable to grasp the ideological spark to the characters that made the musical as such emotional content.\n",
      "rodriguez... being unable to identify the emotional spark to the characters that made the show have such tragic personalities.\n",
      "rodriguez... unfortunately unable to resist the moral spark from the characters that made the film feel such characters work.\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "rodriguez... although unable to convey the special spark on the characters that made the short film such with delight.\n",
      "rodriguez... still unable to convey the special spark from the characters that made the entire film such to delight.\n",
      "rodriguez... absolutely unable to overcome the special spark amongst the characters that made the final film such great delight.\n",
      "rodriguez... being unable to convey the special spark from the characters that made the comedy film such immense delight.\n",
      "rodriguez... absolutely unable to reproduce the comic spark from the characters that made the first film imp this delight.\n",
      "rodriguez... but unable to reproduce the true spark and the characters that made the first film their unexpected delight.\n",
      "rodriguez... practically unable to reproduce the emotional spark creating the characters that made the first film of emotional delight.\n",
      "rodriguez... still unable to reproduce the romantic spark through the characters that made the first film seem such delight.\n",
      "rodriguez... was unable to film the explosive spark from from characters that made the first film with creative delight.\n",
      "rodriguez... was unable to reproduce any special spark — the process that made the resulting film such as unique.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display language model explanation\n",
    "print(build_explanation(lm_explanation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = test[indices[4]]\n",
    "\n",
    "# compute text prediction\n",
    "pred = class_names[predict_fn([text])[0]]\n",
    "alternative = class_names[1 - predict_fn([text])[0]]\n",
    "\n",
    "# similarity explanation\n",
    "sim_explanation = explainer.explain(\n",
    "    text,\n",
    "    sampling_method=\"similarity\",\n",
    "    **config\n",
    ")\n",
    "\n",
    "# language model explanation\n",
    "lm_explanation = explainer.explain(\n",
    "    text,\n",
    "    sampling_method='language_model',\n",
    "    **config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: while the story's undeniably hard to follow , iwai's gorgeous visuals seduce .\n"
     ]
    }
   ],
   "source": [
    "print(\"Original text:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: follow AND undeniably AND seduce\n",
      "Precision: 0.96\n",
      "\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "while both story 's undeniably decent to follow , iwai 's gorgeous visuals seduce .\n",
      "while the tragedy 's undeniably hard to follow , iwai 's gorgeous imaginations seduce .\n",
      "while those epilogue 's undeniably ready to follow , iwai 's dashing graphics seduce .\n",
      "while those memoir is undeniably hungry to follow , iwai 's gorgeous visuals seduce .\n",
      "while these story loses undeniably hard to follow , iwai 's dashing textures seduce .\n",
      "while the story believes undeniably hard to follow , iwai 's gorgeous visuals seduce .\n",
      "while the fact understands undeniably strong to follow , iwai 's brilliant presentations seduce .\n",
      "while this book sees undeniably big to follow , iwai 's sensational visuals seduce .\n",
      "while the story 's undeniably black to follow , iwai 's magnificent visuals seduce .\n",
      "while the cartoonist 's undeniably ready to follow , iwai 's dapper vocals seduce .\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "while some plot 's undeniably hard to follow , iwai 's handsome animators seduce .\n",
      "while this tragedy looses undeniably bad to follow , iwai 's perfect animations seduce .\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display similarity explanation\n",
    "print(build_explanation(sim_explanation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: undeniably AND gorgeous\n",
      "Precision: 0.99\n",
      "\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "while the story's undeniably hard to follow, zoe's gorgeous visuals seduce.\n",
      "while the story's undeniably hard to follow, anastasia's gorgeous visuals seduce.\n",
      "while each story's undeniably hard to follow, nichols's gorgeous visuals seduce.\n",
      "while the story's undeniably hard to follow, james's gorgeous visuals seduce.\n",
      "while another story's undeniably hard to follow, spears's gorgeous visuals seduce.\n",
      "while the story's undeniably hard to follow, lewis's gorgeous visuals seduce.\n",
      "while the story's undeniably hard to follow, roberts's gorgeous visuals seduce.\n",
      "while each story's undeniably hard to follow, luke's gorgeous visuals seduce.\n",
      "while the story's undeniably hard to follow, davies's gorgeous visuals seduce.\n",
      "while its story's undeniably hard to follow, zoe's gorgeous visuals seduce.\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display language model explanation\n",
    "print(build_explanation(lm_explanation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = test[indices[5]]\n",
    "\n",
    "# compute text prediction\n",
    "pred = class_names[predict_fn([text])[0]]\n",
    "alternative = class_names[1 - predict_fn([text])[0]]\n",
    "\n",
    "# similarity explanation\n",
    "sim_explanation = explainer.explain(\n",
    "    text,\n",
    "    sampling_method=\"similarity\",\n",
    "    **config\n",
    ")\n",
    "\n",
    "# language model explanation\n",
    "lm_explanation = explainer.explain(\n",
    "    text,\n",
    "    sampling_method='language_model',\n",
    "    **config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: i have two words to say about reign of fire . great dragons !\n"
     ]
    }
   ],
   "source": [
    "print(\"Original text:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: dragons AND have AND fire\n",
      "Precision: 0.95\n",
      "\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "i have two pronunciations to know about reign with fire . great dragons !\n",
      "i have two words to understand that reign of fire . great dragons !\n",
      "i have two things to bother around caliphate across fire . cool dragons !\n",
      "i have two times to know about kingdom of fire . glad dragons !\n",
      "i have two texts to suppose that figurehead of fire . great dragons !\n",
      "i have two words to see about century of fire . great dragons !\n",
      "i have two words to suppose about reign upon fire . extraordinary dragons !\n",
      "i have two words to give past reign of fire . great dragons !\n",
      "i have two meanings to disagree arround reign of fire . great dragons !\n",
      "i have two words to say before vassal of fire . great dragons !\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "i have two words to realize about plutocracy of fire . great dragons !\n",
      "i have two words to feel without caliphate of fire . happy dragons !\n",
      "i have two words to expect that predecessor without fire . great dragons !\n",
      "i have two words to remember about era amid fire . great dragons !\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display similarity explanation\n",
    "print(build_explanation(sim_explanation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: dragons AND say AND about\n",
      "Precision: 0.97\n",
      "\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "i have two words to say about lots of dragons. great dragons!\n",
      "we have two words to say about dragons of us. great dragons!\n",
      "i have two words to say about gods of dragons. great dragons!\n",
      "i have two words to say about lots of dragons. great dragons!\n",
      "i have two words to say about plenty of dragons. great dragons!\n",
      "dragons have two words to say about shades of dragons. great dragons!\n",
      "we have two words to say about dragons of dragons. great dragons!\n",
      "i have two words to say about monsters of dragons. great dragons!\n",
      "we have two words to say about ser dragons castles. great dragons!\n",
      "we have two words to say about dragons with dragons. great dragons!\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "must have two words to say about creatures of magic. great dragons!\n",
      "i have two words to say about tales of elves. great dragons!\n",
      "wizards have two words to say about our their dragons. great dragons!\n",
      "you have two words to say about those new dragons. great dragons!\n",
      "i have two words to say about reign of terror. great dragons!\n",
      "i have two words to say about reign of terror. great dragons!\n",
      "i have two words to say about reign of terror. great dragons!\n",
      "many from two tales legends say about thousands of dragons. great dragons!\n",
      "this made two different you say about gods of trolls. great dragons!\n",
      "each are two great who say about generations of course. great dragons!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display language model explanation\n",
    "print(build_explanation(lm_explanation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = test[indices[6]]\n",
    "\n",
    "# compute text prediction\n",
    "pred = class_names[predict_fn([text])[0]]\n",
    "alternative = class_names[1 - predict_fn([text])[0]]\n",
    "\n",
    "# similarity explanation\n",
    "sim_explanation = explainer.explain(\n",
    "    text,\n",
    "    sampling_method=\"similarity\",\n",
    "    **config\n",
    ")\n",
    "\n",
    "# language model explanation\n",
    "lm_explanation = explainer.explain(\n",
    "    text,\n",
    "    sampling_method='language_model',\n",
    "    **config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: what's next : \" my mother the car ? \"\n"
     ]
    }
   ],
   "source": [
    "print(\"Original text:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: car AND next\n",
      "Precision: 0.96\n",
      "\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "what 's next : \" my mother this car ? \"\n",
      "what 's next : \" my mother the car ? \"\n",
      "what 's next : \" my mother the car ? \"\n",
      "what 's next : \" my mother the car ? \"\n",
      "what 's next : \" my mother those car ? \"\n",
      "what 's next : \" my mother the car ? \"\n",
      "what 's next : \" my grandmother every car ? \"\n",
      "what 's next : \" my mother the car ? \"\n",
      "what 's next : \" my mother the car ? \"\n",
      "what 's next : \" my mother the car ? \"\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "what 's next : \" my baby the car ? \"\n",
      "what 's next : \" my baby the car ? \"\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display similarity explanation\n",
    "print(build_explanation(sim_explanation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: car\n",
      "Precision: 0.99\n",
      "\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "o's next : \" my mother the car? \"\n",
      "o's next : \" my mother the car? \"\n",
      "jenkins's next : \" my mother the car? \"\n",
      "i's next : \" my mother the car? \"\n",
      "o's next : \" my mother the car? \"\n",
      "o's next : \" my mother the car? \"\n",
      "o's next : \" my mother the car? \"\n",
      "o's next : \" my mother the car? \"\n",
      "o's next : \" my mother the car? \"\n",
      "p's next : \" my mother the car? \"\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "what's best : \" you you my car? \"\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display language model explanation\n",
    "print(build_explanation(lm_explanation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = test[indices[7]]\n",
    "\n",
    "# compute text prediction\n",
    "pred = class_names[predict_fn([text])[0]]\n",
    "alternative = class_names[1 - predict_fn([text])[0]]\n",
    "\n",
    "# similarity explanation\n",
    "sim_explanation = explainer.explain(\n",
    "    text,\n",
    "    sampling_method=\"similarity\",\n",
    "    **config\n",
    ")\n",
    "\n",
    "# language model explanation\n",
    "lm_explanation = explainer.explain(\n",
    "    text,\n",
    "    sampling_method='language_model',\n",
    "    **config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: supposedly based upon real , or at least soberly reported incidents , the film ends with a large human tragedy . alas , getting there is not even half the interest .\n"
     ]
    }
   ],
   "source": [
    "print(\"Original text:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: reported\n",
      "Precision: 0.95\n",
      "\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "apparently calculated upon real , or at least soberly reported incidents , each anthology is with an sized human tragedy . alas , wanting there is not rather half some interest .\n",
      "even associated upon real , or at least instinctively reported thefts , a supermodel ends between a tiny organism tragedy . alas , getting there is not even half the future .\n",
      "supposedly based upon amazing , or behind fewest gleefully reported incidents , a repertoire edges under any large human catastrophe . alas , getting there is not possibly half these interest .\n",
      "reportedly based upon same , or at least soberly reported bombings , the novella ends than a various human bystander . alas , getting there is not apparently half this interest .\n",
      "litterally based upon real , or at fewest uncomfortably reported incidents , the film becomes from a large human uproar . alas , getting there is not even half each interest .\n",
      "mistakenly based upon genuine , or near fewest awkwardly reported officers , all scene ends with the large nature unfairness . alas , getting there is not someway half the interest .\n",
      "latter driven upon incredible , or by least soberly reported investigators , any cinema ends through a large expression tragedy . alas , getting there is not even half every possibility .\n",
      "probably preferred upon real , or through worst blankly reported incidents , any film gets without a large human tragedy . alas , getting there is not even half some interest .\n",
      "apparently considered upon real , or at fewest softly reported investigations , the film starts with each dwarfed human ordeal . alas , letting there is not even half an refinance .\n",
      "latter based upon real , or at fewest soberly reported accidents , the sequel starts along a significant human tragedy . alas , getting there is not just half every interest .\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "obviously associated upon real , or at fewest discreetly reported murders , the film edges with both large human tragedy . alas , thinking there is not even half a interest .\n",
      "supposedly based upon real , or at least soberly reported incidents , the cinema ends with a large human helplessness . alas , coming there is not ever half the interest .\n",
      "supposedly based upon honest , or beside fewest soberly reported incidents , the drama puts with an large human tragedy . alas , bein there is not even half the ownership .\n",
      "supposedly based upon real , or at least gleefully reported incidents , an film ends with another several human fate . alas , bein there is not apparently half the amount .\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display similarity explanation\n",
    "print(build_explanation(sim_explanation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: getting AND large AND half AND the AND there\n",
      "Precision: 0.98\n",
      "\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "supposedly based upon evidence, or by or or reported incidents, the episode ends with a large human tragedy. finally, getting there does hurts even half the participants.\n",
      "supposedly based upon predictions, or after by often reported incidents, the game ends with a large human tragedy. unfortunately, getting there requires causes even half the tragedy.\n",
      "supposedly based on reports, or other random or reported incidents, the festival ends with a large human tragedy. hence, getting there requires destroys even half the risk.\n",
      "supposedly based on facts, or even and newly reported incidents, the story ends with another large human tragedy. however, getting there requires means even half the consequences.\n",
      "supposedly based upon information, or rather circumstances many reported incidents, the book ends with a large human tragedy. fortunately, getting there is affects even half the story.\n",
      "supposedly based on disasters, or simply about wrongly reported incidents, the story ends with a large human tragedy. indeed, getting there risks takes even half the incident.\n",
      "supposedly based on circumstances, or dec than of reported incidents, the game ends with relatively large human tragedy. otherwise, getting there takes is even half the blame.\n",
      "supposedly based on investigation, or un false from reported incidents, the expedition ends with a large human tragedy. inevitably, getting there sucks almost even half the cost.\n",
      "supposedly based on rumors, or based the accurately reported incidents, the adventure ends with a large human tragedy. unfortunately, getting there risked hurt even half the problem.\n",
      "supposedly based upon witnesses, and at least four accidental incidents, the locals start with its large ransom crowds. unfortunately, getting there was not even half the number.\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "supposedly based on circumstances, or based any accurately reported incidents, the storyline ends with a large human tragedy. however, getting there rarely costs even half the score.\n",
      "generally based upon real, or hypothetical accurate commercially reported findings, 3d film screenings generated caused large stakes budgets. however, getting there gives attracted even half the interest.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display language model explanation\n",
    "print(build_explanation(lm_explanation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = test[indices[8]]\n",
    "\n",
    "# compute text prediction\n",
    "pred = class_names[predict_fn([text])[0]]\n",
    "alternative = class_names[1 - predict_fn([text])[0]]\n",
    "\n",
    "# similarity explanation\n",
    "sim_explanation = explainer.explain(\n",
    "    text,\n",
    "    sampling_method=\"similarity\",\n",
    "    **config\n",
    ")\n",
    "\n",
    "# language model explanation\n",
    "lm_explanation = explainer.explain(\n",
    "    text,\n",
    "    sampling_method='language_model',\n",
    "    **config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: a well-crafted letdown .\n"
     ]
    }
   ],
   "source": [
    "print(\"Original text:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: crafted AND well AND a\n",
      "Precision: 0.98\n",
      "\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "a well - crafted disdain .\n",
      "a well - crafted incredulity .\n",
      "a well - crafted concern .\n",
      "a well - crafted angsty .\n",
      "a well - crafted apathy .\n",
      "a well - crafted letdown .\n",
      "a well - crafted lament .\n",
      "a well - crafted letdown .\n",
      "a well - crafted letdown .\n",
      "a well - crafted letdown .\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "a well - crafted embarrassment .\n",
      "a well - crafted embarrassment .\n",
      "a well - crafted embarrassment .\n",
      "a well - crafted disappointment .\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display similarity explanation\n",
    "print(build_explanation(sim_explanation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: crafted AND well AND a AND letdown\n",
      "Precision: 1.00\n",
      "\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "a well - crafted letdown.\n",
      "a well - crafted letdown.\n",
      "a well - crafted letdown.\n",
      "a well - crafted letdown.\n",
      "a well - crafted letdown.\n",
      "a well - crafted letdown.\n",
      "a well - crafted letdown.\n",
      "a well - crafted letdown.\n",
      "a well - crafted letdown.\n",
      "a well - crafted letdown.\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display language model explanation\n",
    "print(build_explanation(lm_explanation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = test[indices[9]]\n",
    "\n",
    "# compute text prediction\n",
    "pred = class_names[predict_fn([text])[0]]\n",
    "alternative = class_names[1 - predict_fn([text])[0]]\n",
    "\n",
    "# similarity explanation\n",
    "sim_explanation = explainer.explain(\n",
    "    text,\n",
    "    sampling_method=\"similarity\",\n",
    "    **config\n",
    ")\n",
    "\n",
    "# language model explanation\n",
    "lm_explanation = explainer.explain(\n",
    "    text,\n",
    "    sampling_method='language_model',\n",
    "    **config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: it's just hard to believe that a life like this can sound so dull .\n"
     ]
    }
   ],
   "source": [
    "print(\"Original text:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: dull\n",
      "Precision: 1.00\n",
      "\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "it 's just hard to believe that a relationship without this can sound so dull .\n",
      "it 's just hard to believe that a thing past this can sound not dull .\n",
      "it 's just painful to beleive that a death around this can sing really dull .\n",
      "it 's just hard to need that every life like some can sound so dull .\n",
      "it 's just hard to believe that a family although an can sound only dull .\n",
      "it 's just wrong to believe that a society although the can squeal well dull .\n",
      "it 's somehow big to consider that the life like the can sound pretty dull .\n",
      "it 's just nasty to believe that a world like this can sound so dull .\n",
      "it 's so bad to doubt that a life like both can sound almost dull .\n",
      "it 's just nasty to suppose that a life like this can hear so dull .\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display similarity explanation\n",
    "print(build_explanation(sim_explanation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: dull\n",
      "Precision: 1.00\n",
      "\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "it's the you i believe that a life like ours can be so dull.\n",
      "it's rare people you believe that a life like that can be so dull.\n",
      "it's funny everyone anyone believe that a life like this can be so dull.\n",
      "it's not to you believe that a life like this can look so dull.\n",
      "it's the you everybody believe that a life like ours can be so dull.\n",
      "it's funny time you believe that a life like marriage can be so dull.\n",
      "it's not him you believe making a life like yours can be so dull.\n",
      "it's odd the i believe that a life like ours can be so dull.\n",
      "it's why make you believe that a life like yours can be so dull.\n",
      "it's true him you believe that a life like this can get so dull.\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display language model explanation\n",
    "print(build_explanation(lm_explanation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alibi",
   "language": "python",
   "name": "alibi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
