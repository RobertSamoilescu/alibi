{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import spacy\n",
    "import string\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from alibi.explainers import AnchorText\n",
    "from alibi.datasets import fetch_movie_sentiment\n",
    "from alibi.utils.download import spacy_model\n",
    "from alibi.utils.lang_model import DistilbertBaseUncased, BertBaseUncased, RobertaBase\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load movie review dataset\n",
    "\n",
    "The `fetch_movie_sentiment` function returns a `Bunch` object containing the features, the targets and the target names for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = fetch_movie_sentiment()\n",
    "movies.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = movies.data\n",
    "labels = movies.target\n",
    "target_names = movies.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, train_labels, test_labels = train_test_split(data, labels, test_size=.2, random_state=42)\n",
    "train, val, train_labels, val_labels = train_test_split(train, train_labels, test_size=.1, random_state=42)\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "val_labels = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply CountVectorizer to training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=1)\n",
    "vectorizer.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "clf = LogisticRegression(solver='liblinear')\n",
    "clf.fit(vectorizer.transform(train), train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fn = lambda x: clf.predict(vectorizer.transform(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.9801624284382905\n",
      "Validation accuracy 0.7544910179640718\n",
      "Test accuracy 0.7589841878294202\n"
     ]
    }
   ],
   "source": [
    "preds_train = predict_fn(train)\n",
    "preds_val = predict_fn(val)\n",
    "preds_test = predict_fn(test)\n",
    "print('Train accuracy', accuracy_score(train_labels, preds_train))\n",
    "print('Validation accuracy', accuracy_score(val_labels, preds_val))\n",
    "print('Test accuracy', accuracy_score(test_labels, preds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load spaCy model\n",
    "\n",
    "English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Assigns word vectors, context-specific token vectors, POS tags, dependency parse and named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'en_core_web_md'\n",
    "spacy_model(model=model)\n",
    "nlp = spacy.load(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForMaskedLM: ['activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "lang_model = DistilbertBaseUncased()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 299  379  997 1601  264  229 1774 1968  458  252]\n"
     ]
    }
   ],
   "source": [
    "indices = np.random.choice(len(test), size=10, replace=False)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_explanation(explanation) -> str:\n",
    "    s = ''\n",
    "    s += 'Anchor: %s\\n' % (' AND '.join(explanation.anchor))\n",
    "    s += 'Precision: %.2f\\n' % explanation.precision\n",
    "    \n",
    "    # print examples covered as True\n",
    "    s += '\\n\\nExamples where anchor applies and model predicts %s:\\n' % pred\n",
    "    if len(explanation.raw['examples']):\n",
    "        s += '\\n'.join([x for x in explanation.raw['examples'][-1]['covered_true']])\n",
    "    \n",
    "    # print examples covered as False\n",
    "    s += '\\n\\nExamples where anchor applies and model predicts %s:\\n' % alternative\n",
    "    if len(explanation.raw['examples']):\n",
    "        s += '\\n'.join([x for x in explanation.raw['examples'][-1]['covered_false']])\n",
    "    \n",
    "    s += '\\n\\n\\n'\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_similarity = {\n",
    "    \"sample_proba\": [0.5],\n",
    "    \"punctuation\": [string.punctuation],\n",
    "    \"top_n\": [10, 100, 500],\n",
    "    \"sampling_method\": [\"similarity\"]\n",
    "}\n",
    "\n",
    "config_language_model = {\n",
    "    \"sample_proba\": [0.5],\n",
    "    \"filling_method\": ['parallel'],\n",
    "    \"punctuation\": [string.punctuation],\n",
    "    \"top_n\": [10, 100, 500],\n",
    "    \"frac_mask_templates\": [0.1, 0.5, 1.0],\n",
    "    \"sampling_method\": [\"language_model\"],\n",
    "    \"batch_size_lm\": [64]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = movies.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_configuration(tests: dict, file_name: str) -> dict:\n",
    "    # get all combinations\n",
    "    values = tests.values()\n",
    "    combinations = itertools.product(*values)\n",
    "    \n",
    "    # open checkpoint file\n",
    "    f = open(file_name, 'wb')\n",
    "    configs = []\n",
    "\n",
    "    for comb in tqdm(combinations):\n",
    "        # build configuration\n",
    "        config = dict(zip(tests.keys(), comb))\n",
    "        \n",
    "        # initialize explainer\n",
    "        explainer = AnchorText(nlp=nlp, language_model=lang_model, predictor=predict_fn, **config)\n",
    "        \n",
    "        # set timer\n",
    "        config['elapsed_time'] = 0\n",
    "\n",
    "        for index in tqdm(indices):\n",
    "            text = test[index]\n",
    "\n",
    "            # compute text prediction\n",
    "            pred = class_names[predict_fn([text])[0]]\n",
    "            alternative = class_names[1 - predict_fn([text])[0]]\n",
    "         \n",
    "            # compute explanation\n",
    "            start = time.time()\n",
    "            explanation = explainer.explain(text, threshold=0.95)\n",
    "            config['elapsed_time'] += (time.time() - start)\n",
    "        \n",
    "        # compute mean\n",
    "        config['elapsed_time'] /= len(indices)\n",
    "        configs.append(config)\n",
    "        \n",
    "    # append it to output file\n",
    "    pkl.dump(configs, f)\n",
    "    f.close()\n",
    "    \n",
    "    return configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"stats_similarity.pkl\"):\n",
    "    configs_sim = run_configuration(config_similarity, \"stats_similarity.pkl\")\n",
    "\n",
    "with open(\"stats_similarity.pkl\", \"rb\") as fin:\n",
    "    configs_sim = pkl.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:19<02:51, 19.10s/it]\u001b[A\n",
      " 20%|██        | 2/10 [01:02<04:29, 33.64s/it]\u001b[A\n",
      " 30%|███       | 3/10 [01:38<04:02, 34.70s/it]\u001b[A\n",
      " 40%|████      | 4/10 [02:09<03:19, 33.25s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"stats_lm.pkl\"):\n",
    "    configs_lm = run_configuration(config_language_model, \"stats_lm.pkl\")\n",
    "\n",
    "with open(\"stats_lm.pkl\", \"rb\") as fin:\n",
    "    configs_lm = pkl.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = [], []\n",
    "\n",
    "# collect similarity results\n",
    "for conf in configs_sim:\n",
    "    name = \"%s, top %d\" % (\"SIM\", conf[\"top_n\"])\n",
    "    x.append(name)\n",
    "    y.append(conf[\"elapsed_time\"])\n",
    "\n",
    "# collect language model results\n",
    "for conf in configs_lm:\n",
    "    name = \"%s, top %d, PMT %.2f\" % \\\n",
    "        (\"LM\", conf[\"top_n\"], conf[\"prec_mask_templates\"])\n",
    "    x.append(name)\n",
    "    y.append(conf[\"elapsed_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(range(len(x)), y)\n",
    "plt.xticks(range(len(x)), x)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alibi",
   "language": "python",
   "name": "alibi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
