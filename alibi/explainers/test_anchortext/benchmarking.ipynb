{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import spacy\n",
    "import string\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from alibi.explainers import AnchorText\n",
    "from alibi.datasets import fetch_movie_sentiment\n",
    "from alibi.utils.download import spacy_model\n",
    "from alibi.utils.lang_model import DistilbertBaseUncased, BertBaseUncased, RobertaBase\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load movie review dataset\n",
    "\n",
    "The `fetch_movie_sentiment` function returns a `Bunch` object containing the features, the targets and the target names for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = fetch_movie_sentiment()\n",
    "movies.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = movies.data\n",
    "labels = movies.target\n",
    "target_names = movies.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, train_labels, test_labels = train_test_split(data, labels, test_size=.2, random_state=42)\n",
    "train, val, train_labels, val_labels = train_test_split(train, train_labels, test_size=.1, random_state=42)\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "val_labels = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply CountVectorizer to training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=1)\n",
    "vectorizer.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "clf = LogisticRegression(solver='liblinear')\n",
    "clf.fit(vectorizer.transform(train), train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fn = lambda x: clf.predict(vectorizer.transform(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = predict_fn(train)\n",
    "preds_val = predict_fn(val)\n",
    "preds_test = predict_fn(test)\n",
    "print('Train accuracy', accuracy_score(train_labels, preds_train))\n",
    "print('Validation accuracy', accuracy_score(val_labels, preds_val))\n",
    "print('Test accuracy', accuracy_score(test_labels, preds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load spaCy model\n",
    "\n",
    "English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Assigns word vectors, context-specific token vectors, POS tags, dependency parse and named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'en_core_web_md'\n",
    "spacy_model(model=model)\n",
    "nlp = spacy.load(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_model = DistilbertBaseUncased()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.choice(len(test), size=10, replace=False)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = AnchorText(nlp=nlp, language_model=lang_model, predictor=predict_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_explanation(explanation) -> str:\n",
    "    s = ''\n",
    "    s += 'Anchor: %s\\n' % (' AND '.join(explanation.anchor))\n",
    "    s += 'Precision: %.2f\\n' % explanation.precision\n",
    "    \n",
    "    # print examples covered as True\n",
    "    s += '\\n\\nExamples where anchor applies and model predicts %s:\\n' % pred\n",
    "    if len(explanation.raw['examples']):\n",
    "        s += '\\n'.join([x for x in explanation.raw['examples'][-1]['covered_true']])\n",
    "    \n",
    "    # print examples covered as False\n",
    "    s += '\\n\\nExamples where anchor applies and model predicts %s:\\n' % alternative\n",
    "    if len(explanation.raw['examples']):\n",
    "        s += '\\n'.join([x for x in explanation.raw['examples'][-1]['covered_false']])\n",
    "    \n",
    "    s += '\\n\\n\\n'\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_similarity = {\n",
    "    \"threshold\": [0.95],\n",
    "    \"sample_proba\": [0.5],\n",
    "    \"punctuation\": [string.punctuation],\n",
    "    \"top_n\": [10, 100, 500],\n",
    "    \"sampling_method\": [\"similarity\"]\n",
    "}\n",
    "\n",
    "config_language_model = {\n",
    "    \"threshold\": [0.95],\n",
    "    \"sample_proba\": [0.5],\n",
    "    \"filling_method\": ['parallel'],\n",
    "    \"punctuation\": [string.punctuation],\n",
    "    \"top_n\": [10, 100, 500],\n",
    "    \"prec_mask_templates\": [0.1, 0.5, 1.0],\n",
    "    \"sampling_method\": [\"language_model\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = movies.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_configuration(tests: dict, file_name: str) -> dict:\n",
    "    # get all combinations\n",
    "    values = tests.values()\n",
    "    combinations = itertools.product(*values)\n",
    "    \n",
    "    # open checkpoint file\n",
    "    f = open(file_name, 'wb')\n",
    "    configs = []\n",
    "\n",
    "    for comb in tqdm(combinations):\n",
    "        # build configuration\n",
    "        config = dict(zip(tests.keys(), comb))\n",
    "        config['elapsed_time'] = 0\n",
    "\n",
    "        for index in tqdm(indices):\n",
    "            text = test[index]\n",
    "\n",
    "            # compute text prediction\n",
    "            pred = class_names[predict_fn([text])[0]]\n",
    "            alternative = class_names[1 - predict_fn([text])[0]]\n",
    "         \n",
    "            # define explainer\n",
    "            np.random.seed(0)\n",
    "            explainer = AnchorText(nlp=nlp, language_model=lang_model, predictor=predict_fn)\n",
    "\n",
    "            # compute explanation\n",
    "            start = time.time()\n",
    "            explanation = explainer.explain(text, **config)\n",
    "            config['elapsed_time'] += (time.time() - start)\n",
    "        \n",
    "        # compute mean\n",
    "        config['elapsed_time'] /= len(indices)\n",
    "        configs.apppend(config)\n",
    "        \n",
    "    # append it to output file\n",
    "    pkl.dump(configs, f)\n",
    "    f.close()\n",
    "    \n",
    "    return configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_sim = run_configuration(config_similarity, \"stats_similarity.pkl\")\n",
    "configs_lm = run_configuration(config_language_model, \"stats_lm.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alibi",
   "language": "python",
   "name": "alibi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
