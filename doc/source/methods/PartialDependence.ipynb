{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf2856d4",
   "metadata": {},
   "source": [
    "[[source]](../api/alibi.explainers.html#alibi.explainers.PartialDependence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02e3cdb",
   "metadata": {},
   "source": [
    "# Partial Dependence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7f776f",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17064c71",
   "metadata": {},
   "source": [
    "The partial dependence (PD) plot proposed by [J.H. Friedman (2001)](https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boostingmachine/10.1214/aos/1013203451.full) , is a method of visualizing the marginal effect that one or two features have on the predicted outcome of a machine learning model. By inspecting the PD plots, one can understand whether the relation between a feature/pair of features is, for example, a simple linear or quadratic relation, whether it presents a monotonically increasing or decreasing trend, or reveal a more complex response.\n",
    "\n",
    "Before diving into the mathematical formulation of the PD, we first introduce some notation. Consider $\\mathcal{F}$ to be the set of all features, $S$ be a set of features of interest (i.e. $S \\subseteq \\mathcal{F}$) that we want to compute the marginal effect for, and $C$ be their complement (i.e. $C = \\mathcal{F} \\setminus S$). It is important to note that the subset $S$ is not only restricted to one or two features as mentioned in the previous paragraph, but can be any subset of the set $\\mathcal{F}$. In practice, though, due to visualization reasons, we will not analyze more than two features at the time.\n",
    "\n",
    "Given a black-box model, $f$, we are ready to define the partial dependence for a set of features $S$ as:\n",
    "\n",
    "$$\n",
    "    f_{S}(x_S) = \\mathbb{E}_{X_C} [f(x_S, X_C)] = \\int f(x_S, X_C) d\\mathbb{P}(X_C),\n",
    "$$\n",
    "\n",
    "where we denoted random variables by capital letters (e.g., $X_C$), realizations by lowercase letters (e.g. $x_S$), and $\\mathbb{P}(X_C)$ the probability distribution/measure over the features set C. \n",
    "\n",
    "In practice, to approximate the integral above, we rely on Monte Carol method, computing the average over a reference dataset. Formally, let us consider a reference dataset $\\mathcal{X} = \\{x^{(1)}, x^{(2)}, ..., x^{(n)}\\}$. The PD for the set $S$ can be approximated as:\n",
    "\n",
    "$$\n",
    "f_{S}(x_S) = \\frac{1}{n} \\sum_{i=1}^{n}f(x_S, x_{C}^{(i)}).\n",
    "$$\n",
    "\n",
    "In simple words, to compute the marginal effect of the feature values $x_S$, we query the model on synthetic instance created from the concatenation of $x_S$ with the feature values $x_C^{(i)}$ from the reference dataset, and averaging the model responses. Already that from the computation we can identify a major limitation of the method. The PD computation assumes feature independence (i.e. feature not correlated) which is a strong and quite restrictive assumption and usually does not hold in practice. A classic example of positively correlated features is *height* and *weight*. If $S = \\{\\text{height}\\}$ and $C=\\{\\text{weight}\\}$, the independence assumption will create unrealistic synthetic instance, by combining values of *height* and *weight* that do not correlated (e.g. $\\text{height} = 1.85\\text{m}$ - hight of an adult - and $\\text{weight} = 30 \\text{kg}$ - weight of child). Such limitation can be addressed by ALE.\n",
    "\n",
    "Although the ALE can handle correlated features, the PD still have some advantages beyond their simple and intuitive definition. The PD can directly be extended to categorical features. For each category of a feature, one can compute the PD by setting all data instance to have the same category and following the same averaging strategy over the reference dataset (i.e. replace features $C$ with feature values from the reference, compute the response, and average all the responses). Note that ALE requires by definition the feature values to be ordinal, which might not be the case for all categorical features. Depending on the context, there exists some methods that allow the ALE to be extended to categorical feature, for which we recommend the [ALE chapter](https://christophm.github.io/interpretable-ml-book/ale.html) from [Interpretable machine learning](https://christophm.github.io/interpretable-ml-book/) book, C. Molnar(2020) as further reading.\n",
    "\n",
    "We will continue to discuss the simple case of linear regression. Note that for linear regression, the PD will always show a linear relationship between the features and the response. To formally prove the linear relationship, consider the following linear regression model:\n",
    "\n",
    "$$\n",
    "f(x) = \\beta_0 + \\beta_1 x_1 + ... \\beta_{|\\mathcal{F}|} x_{|\\mathcal{F}|}.\n",
    "$$\n",
    "\n",
    "Without loss of generality, we can assume the the features of interest come first in the equation above and the rest of the features at the end. We can rewrite the above equation as follows:\n",
    "\n",
    "$$\n",
    "f(x) = \\beta_0 + \\beta_1 x_{S_1} + ... + \\beta_{|S|} x_{S_{|S|}} + \\beta_{|S| + 1} x_{C_{1}} + ... + \\beta_{|S| + |C|} x_{C_{|C|}},\n",
    "$$\n",
    "\n",
    "where $S_{i}$ and $C_{i}$ represent features in $S$ and $C$, respectively.\n",
    "\n",
    "Following the definition of PD, we obtain:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    & f_S(x_{S}) = \\mathbb{E}_{X_C}[f(x_S, X_C)] \\\\\n",
    "    &  =  \\mathbb{E}_{X_C}[\\beta_0 + \\beta_1 x_{S_1} + ... + \\beta_{|S|} x_{S_{|S|}} + \\beta_{|S| + 1} x_{C_{1}} + ... + \\beta_{|S| + |C|} x_{C_{|C|}}] \\\\\n",
    "    & = \\beta_0 + \\beta_1 x_{S_1} + ... + \\beta_{|S|} x_{S_{|S|}} + \\mathbb{E}_{X_C}[x_{C_{1}} + ... + \\beta_{|S| + |C|} x_{C_{|C|}}] \\\\\n",
    "    & = \\beta_0 + \\beta_1 x_{S_1} + ... + \\beta_{|S|} x_{S_{|S|}} + K_C \\\\\n",
    "    & = (\\beta_0 + K_C) + \\beta_1 x_{S_1} + ... + \\beta_{|S|} x_{S_{|S|}},\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "where $K_C = \\mathbb{E}_{X_C}[x_{C_{1}} + ... + \\beta_{|S| + |C|} x_{C_{|C|}}]$.\n",
    "Thus we can see that the PD $f_S(X_S)$ is linear in the features in $S$.\n",
    "\n",
    "For further readings, we strongly recommend the [PDP chapter](https://christophm.github.io/interpretable-ml-book/pdp.html) from the [Interpretable machine learning](https://christophm.github.io/interpretable-ml-book/) book, C. Molnar (2020).\n",
    "\n",
    "The PD plot shows the marginal effect of a features and thus is a global method (i.e., it does not focus on a particular instance, but average the response over multiple synthetic data instances). The global nature of the PD hides inevitably some heterogeneous effect by feature interactions. To reveal the heterogeneous relationship, one can plot the individual conditional expectation (ICE). Following the definitions from [ICE chapter], C. Molnar (2020), an ICE plot displays the dependence of the prediction on a feature (i.e. due to visualization constraints it is restricted to a single feature) for each instance separately which results in a plot per data instance. Formally, given the reference dataset $\\mathcal{X}=\\{(x_{S}^{(i)}, x_{C}^{(i)}\\}_{i=1}^{N}$ and a black box-model $f$, the ICE plot will display a curve $f^{(i)}$ plotted against $\\{x_{S}^{(j)}\\}_{j=1}^{N}$ while keeping $x_{C}^{(i)}$ fixed.\n",
    "\n",
    "For further readings, we strongly recommend the [ICE chapter](https://christophm.github.io/interpretable-ml-book/ice.html) from the [Interpretable machine learning](https://christophm.github.io/interpretable-ml-book/) book, C. Molnar (2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c716a8",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5d28b8",
   "metadata": {},
   "source": [
    "To initialize the explainer with a `sklearn` model one can directly pass the estimator and optionally a list of feature names, target names and categorical names for interpretation and specification of the categorical features\n",
    "\n",
    "```python\n",
    "from alibi.explainers import PartialDependence\n",
    "pd = PartialDependence(predictor=predictor,\n",
    "                       feature_names=feature_names,\n",
    "                       categorical_names=categorical_names,\n",
    "                       target_names=target_names)\n",
    "```\n",
    "\n",
    "Note that the direct support of the `sklearn` is a consequence of reusing some functionalities provided by `sklearn` when building the `alibi` PD explainer.\n",
    "\n",
    "In addition, the `alibi` PD explainer supports any black-box model by simply providing the prediction function of the model. To use a prediction function, additional meta data regarding the prediction function should be provided through the `predictor_kw` argument.\n",
    "\n",
    "For a classifier predicting with 3 classes, the initialization is:\n",
    "\n",
    "```python\n",
    "pd = PartialDependence(predictor=predictor_fn, \n",
    "                       feature_names=feature_names,\n",
    "                       categorical_names=categorical_names,\n",
    "                       target_names=target_names,\n",
    "                       predictor_kw={\n",
    "                           'predictor_type': 'classifier', \n",
    "                           'prediction_fn': 'predict_proba',   # can be 'decision_function'\n",
    "                           'num_classes': 3\n",
    "                       })\n",
    "```\n",
    "\n",
    "For a regressor, the initialization is:\n",
    "\n",
    "```python\n",
    "pd = PartialDependence(predictor=predictor_fn, \n",
    "                       feature_names=feature_names,\n",
    "                       categorical_names=categorical_names,\n",
    "                       target_names=target_names,\n",
    "                       predictor_kw={\n",
    "                           'predictor_type': 'regressor', \n",
    "                           'prediction_fn': 'predict',   # can be 'decision_function'\n",
    "                       })\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53dc470",
   "metadata": {},
   "source": [
    "Following the initialization, we can produce an explanation given a reference dataset $X$:\n",
    "\n",
    "```python\n",
    "exp = explainer.explain(X=X,\n",
    "                        features_list=feature_list,\n",
    "                        kind='average',\n",
    "                        method='brute')\n",
    "```\n",
    "\n",
    "Multiple arguments can be provided to the explain method:\n",
    "\n",
    "- `X` - An `N x F` reference tabular dataset used to calculate partial dependence curves. This is typically the training dataset or a representative sample.\n",
    "\n",
    "- `features_list` - An optional list of features or pairs of features for which to calculate the partial dependence for.If not provided, the partial dependence will be computed for every single features in the dataset.\n",
    "\n",
    "- `response_method` - Specifies the prediction function to be used. For a classifier it specifies whether to use the `predict_proba` or the `decision_function`. For a regressor, the parameter is ignored. If set to `auto`, the `predict_proba` is tried first, and if not supported then it reverts to `decision_function`. Note that if `method='recursion'`, the prediction function always uses `decision_function`.\n",
    "\n",
    "- `method` - The method used to calculate the average predictions\n",
    "\n",
    "    * ``'recursion'`` - a faster alternative only supported by some tree-based model. For a classifier, the \n",
    "    target response is always the decision function and NOT the predicted probabilities. Furthermore, since\n",
    "    the ``'recursion'`` method computes implicitly the average of the individual conditional expectation\n",
    "    (ICE) by design, it is incompatible with ICE and the `kind` parameter must be set to ``'average'``.\n",
    "    Check the [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.partial_dependence.html#sklearn.inspection.partial_dependence) for a list of supported tree-based classifiers.\n",
    "\n",
    "    * ``'brute'`` - supported for any black-box prediction model, but is more computationally intensive.\n",
    "    \n",
    "    * ``'auto'`` - uses ``'recursion'`` if the `predictor` supports it. Otherwise, uses the ``'brute'`` method.\n",
    "\n",
    "- `kind` - If set to ``'average'``, then only the partial dependence (PD) averaged across all samples from the dataset is returned. If set to ``individual``, then only the individual conditional expectation (ICE) is returned for each individual from the dataset. Otherwise, if set to ``'both'``, then both the PD and the ICE are returned. Note that for the faster ``method='recursion'`` option the only compatible parameter value is ``kind='average'``. To plot the ICE, consider using the more computation intensive ``method='brute'``.\n",
    "\n",
    "- `percentiles` - Lower and upper percentiles used to create extreme values which can potential remove outliers in low density regions. The values must be in [0, 1].\n",
    "\n",
    "- `grid_resolution` - Number of equidistant points to split the range of each target feature. Only applies if the number of unique values of a target feature in the reference dataset `X` is less than the `grid_resolution` value.\n",
    "\n",
    "- `grid_points` - Custom grid points. Must be a `dict` where the keys are the target features indices and the values are monotonically increasing `numpy` arrays defining the grid points for numerical feature, and a subset of categorical feature values for a categorical feature. If the `grid_points` are not specified, then the grid will be constructed based on the unique target feature values available in the reference dataset `X`, or based on the `grid_resolution` and `percentiles` (check `grid_resolution` to see when it applies). For categorical features, the corresponding value in the `grid_points` can be specified either as `numpy` array of strings or `numpy` array of integers corresponding the label encodings. Note that the label encoding must match the ordering of the values provided in the `categorical_names`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bc33b7",
   "metadata": {},
   "source": [
    "The result exp in `Explanation` object which contains the following data-related attributes:\n",
    "\n",
    "- `feature_values` - A list of arrays or list of arrays containing the evaluation points for each explained feature passed in the `feature_list` argument (see `explain` method).\n",
    " \n",
    "- `feature_names` - A list of strings or tuples of string containing the names associated with the explained features elements from `feature_values`.\n",
    "\n",
    "- `feature_deciles` - a list of arrays (one for each numerical features) of the explained feature deciles.\n",
    "\n",
    "- `pd_values` - a list of arrays of PD values (one for each feature/pair of features). Each array has a shape of `T x (V1 x V2 x ...)`, where `T` is the number of target outputs, and `Vi` is the number of evaluation points for the corresponding feature `fi`.\n",
    "\n",
    "- `ice_values` - a list of arrays of ICE values (one for each feature/pair of feature). Each array has a shape of `T x N x (V1 x V2 x ...)`, where `T` is the number of target outputs, `N` is the number of instances in the reference dataset, and `Vi` is the number of evaluation points for the corresponding feature `fi`.\n",
    "\n",
    "- `meta` - Dictionary containing the following metadata:\n",
    "\n",
    "    - `feature_names`, `categorical_names`, `target_names` - See `__init__` method.\n",
    "\n",
    "    - `response_method`, `method` - See `explain` method. The value might have changed based on some internal logic.\n",
    "\n",
    "    - `kind` - See `explain` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05b82d1",
   "metadata": {},
   "source": [
    "Plotting the `pd_values` and `ice_values` against `exp_feature_values` recovers the PD and the ICE plots, respectively. For convenience we included a plotting function `plot_pd` which automatically produces PD and ICE plots using `matplotlib` and `seaborn`.\n",
    "\n",
    "```python\n",
    "from alibi.explainers import plot_pd\n",
    "plot_pd(exp)\n",
    "```\n",
    "\n",
    "The following are the one way PD plots for a random forest regression trained on the [Bike rental](http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset) dataset (see worked [example](../examples/pdp_regression_bike.ipynb)).\n",
    "\n",
    "<img src=\"pdp_one_bike.png\" alt=\"PD plots, one feature, Bike rental dataset.\" width=\"1000\"/>\n",
    "\n",
    "The following are the ICE plots for a random forest regression trained on the [Bike rental](http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset) dataset (see worked [example](../examples/pdp_regression_bike.ipynb)).\n",
    "\n",
    "<img src=\"ice_bike.png\" alt=\"ICE plots, one feature, Bike rental dataset.\" width=\"1000\"/>\n",
    "\n",
    "\n",
    "The following are the two way PD plots for a random forest regression trained on the [Bike rental](http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset) dataset (see worked [example](../examples/pdp_regression_bike.ipynb)).\n",
    "\n",
    "<img src=\"pdp_two_bike.png\" alt=\"PD plots, two features, Bike rental dataset.\" width=\"1000\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71ec929",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "[PD, ICE regression example (Bike rental)](../examples/pdp_regression_bike.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
