{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robert/anaconda3/envs/dl/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from typing import List, Tuple, Dict, Callable\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from alibi.datasets import fetch_adult\n",
    "from alibi.models.pytorch.autoencoder import HeAE\n",
    "from alibi.models.pytorch.actor_critic import Actor, Critic\n",
    "from alibi.models.pytorch.cfrl_models import ADULTEncoder, ADULTDecoder\n",
    "from alibi.models.pytorch.metrics import AccuracyMetric\n",
    "\n",
    "from alibi.explainers.cfrl_tabular import CounterfactualRLTabular\n",
    "from alibi.explainers.cfrl_base import CounterfactualRLBase, ExperienceCallback, TrainingCallback\n",
    "from alibi.explainers.backends.cfrl_tabular import get_he_preprocessor, get_statistics, \\\n",
    "    get_conditional_vector, apply_category_mapping\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train black-box classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch adult dataset\n",
    "adult = fetch_adult()\n",
    "\n",
    "# separate columns in numerical and categorical\n",
    "categorical_names = [adult.feature_names[i] for i in adult.category_map.keys()]\n",
    "categorical_ids = list(adult.category_map.keys())\n",
    "\n",
    "numerical_names = [name for i, name in enumerate(adult.feature_names) if i not in adult.category_map.keys()]\n",
    "numerical_ids = [i for i in range(len(adult.feature_names)) if i not in adult.category_map.keys()]\n",
    "\n",
    "# split data into train and test\n",
    "X, Y = adult.data, adult.target\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessor\n",
    "num_transf = StandardScaler()\n",
    "cat_transf = OneHotEncoder(\n",
    "    categories=[range(len(x)) for x in adult.category_map.values()],\n",
    "    handle_unknown=\"ignore\"\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_transf, numerical_ids),\n",
    "        (\"cat\", cat_transf, categorical_ids)\n",
    "    ],\n",
    "    sparse_threshold=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.fit(X_train)\n",
    "X_train_ohe = preprocessor.transform(X_train)\n",
    "X_test_ohe = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=15, min_samples_split=10, n_estimators=50,\n",
       "                       random_state=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth=15, min_samples_split=10, n_estimators=50, random_state=0)\n",
    "clf.fit(X_train_ohe, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.862\n"
     ]
    }
   ],
   "source": [
    "# define prediction function\n",
    "predictor = lambda x: clf.predict_proba(preprocessor.transform(x))\n",
    "\n",
    "# compute accuracy\n",
    "acc = accuracy_score(y_true=Y_test, y_pred=np.argmax(predictor(X_test), axis=1))\n",
    "print(\"Accuracy: %.3f\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input dimension\n",
    "input_dim = 57\n",
    "\n",
    "# define hidden dim\n",
    "hidden_dim = 128\n",
    "\n",
    "# define latent dimension\n",
    "latent_dim = 15\n",
    "\n",
    "# output dims\n",
    "output_dims = [len(numerical_ids)]\n",
    "output_dims += [len(adult.category_map[cat_id]) for cat_id in categorical_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n"
     ]
    }
   ],
   "source": [
    "# define autoencoder\n",
    "he_ae = HeAE(encoder=ADULTEncoder(hidden_dim=hidden_dim, latent_dim=latent_dim), \n",
    "             decoder=ADULTDecoder(hidden_dim=hidden_dim, output_dims=output_dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add numerical loss\n",
    "he_loss = [nn.MSELoss()]\n",
    "he_loss_weights = [1.]\n",
    "\n",
    "# add categorical losses\n",
    "for i in range(len(categorical_names)):\n",
    "    he_loss.append(nn.CrossEntropyLoss())\n",
    "    he_loss_weights.append(1./len(categorical_names))\n",
    "    \n",
    "# add metrics\n",
    "metrics = {}\n",
    "for i, cat_name in enumerate(categorical_names):\n",
    "    metrics.update({f\"output_{i+2}\": AccuracyMetric()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "he_ae.compile(optimizer=torch.optim.Adam(he_ae.parameters(), lr=1e-3), \n",
    "              loss=he_loss, \n",
    "              loss_weights=he_loss_weights,\n",
    "              metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# Define attribute types, required for datatype conversion.\n",
    "feature_types = {\"Age\": int, \"Capital Gain\": int, \"Capital Loss\": int, \"Hours per week\": int}\n",
    "\n",
    "# define data preprocessor and inverse preprocessor\n",
    "ae_preprocessor, ae_inv_preprocessor = get_he_preprocessor(X=X_train,\n",
    "                                                           feature_names=adult.feature_names,\n",
    "                                                           category_map=adult.category_map,\n",
    "                                                           feature_types=feature_types)\n",
    "\n",
    "# transform to ohe\n",
    "X_trian_ohe = ae_preprocessor(X_train)\n",
    "X_test_ohe = ae_preprocessor(X_test)\n",
    "\n",
    "# define train loader\n",
    "trainset_input = torch.tensor(X_train_ohe).float()\n",
    "trainset_outputs = [torch.tensor(X_train_ohe).float()[:, :len(numerical_ids)]]\n",
    "\n",
    "for cat_id in categorical_ids:\n",
    "    trainset_outputs.append(torch.tensor(X_train[:, cat_id]).long())       \n",
    "\n",
    "trainset = TensorDataset(trainset_input, *trainset_outputs)\n",
    "trainloader = DataLoader(trainset,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         num_workers=NUM_WORKERS,\n",
    "                         shuffle=True,\n",
    "                         drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "he_ae_dir = \"pytorch/he_autoencoder/\"\n",
    "he_ae_path = os.path.join(he_ae_dir, \"he_autoencoder_adult.pt\")\n",
    "\n",
    "if not os.path.exists(he_ae_dir):\n",
    "    os.makedirs(he_ae_dir)\n",
    "\n",
    "if not os.path.exists(he_ae_path):\n",
    "    he_ae.fit(trainloader, epochs=50)\n",
    "    he_ae.save_weights(he_ae_path)\n",
    "else:\n",
    "    # load the model\n",
    "    he_ae.load_weights(he_ae_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 45,   4,   4,   0,   6,   0,   2,   1, 166,  14,  59,   9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from alibi.explainers.backends.pytorch.cfrl_base import to_numpy\n",
    "X_train_hat_ohe = np.concatenate(to_numpy(he_ae(torch.tensor(X_train_ohe).float().cuda())), axis=1)\n",
    "X_train_hat = ae_inv_preprocessor(X_train_hat_ohe)\n",
    "X_train_hat[0].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002064818837136055"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((X_train_ohe[:, :4] - X_train_hat_ohe[:, :4])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counterfactual RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define dataset specifi attributes and constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define immutable features\n",
    "immutable_features = ['Marital Status', 'Relationship', 'Race', 'Sex']\n",
    "\n",
    "# define ranges\n",
    "ranges = {'Age': [-0.0, 1.0]}\n",
    "\n",
    "\n",
    "# compute statistic for clamping\n",
    "stats = get_statistics(X=X_train, \n",
    "                       preprocessor=ae_preprocessor, \n",
    "                       category_map=adult.category_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define training callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardCallback(ExperienceCallback):\n",
    "    def __call__(self,\n",
    "                 step: int, \n",
    "                 update: int, \n",
    "                 model: CounterfactualRLBase,\n",
    "                 sample: Dict[str, np.ndarray],\n",
    "                 losses: Dict[str, float]):\n",
    "        \n",
    "        if (step + update) % 100 != 0:\n",
    "            return\n",
    "        \n",
    "        # get the counterfactual and target\n",
    "        Y_t = sample[\"Y_t\"]\n",
    "        X_cf = sample[\"X_cf\"]\n",
    "        \n",
    "        # get prediction label\n",
    "        Y_m_cf = predictor(X_cf)\n",
    "        \n",
    "        # compute reward\n",
    "        reward = np.mean(model.params[\"reward_func\"](Y_m_cf, Y_t))\n",
    "        wandb.log({\"reward\": reward})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisplayLossCallback(TrainingCallback):\n",
    "    def __call__(self,\n",
    "                 step: int, \n",
    "                 update: int, \n",
    "                 model: CounterfactualRLBase,\n",
    "                 sample: Dict[str, np.ndarray],\n",
    "                 losses: Dict[str, float]):\n",
    "        # log training losses\n",
    "        if (step + update) % 100 == 0:\n",
    "            wandb.log(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ddpg\n",
    "explainer = CounterfactualRLTabular(encoder=he_ae.encoder,\n",
    "                                    decoder=he_ae.decoder,\n",
    "                                    latent_dim=latent_dim,\n",
    "                                    encoder_preprocessor=ae_preprocessor,\n",
    "                                    decoder_inv_preprocessor=ae_inv_preprocessor,\n",
    "                                    predictor=predictor,\n",
    "                                    coeff_sparsity=0.5,\n",
    "                                    coeff_consistency=0.5,\n",
    "                                    category_map=adult.category_map,\n",
    "                                    feature_names=adult.feature_names,\n",
    "                                    ranges=ranges,\n",
    "                                    immutable_features=immutable_features,\n",
    "                                    train_callbacks=[DisplayLossCallback()], #, RewardCallback()],\n",
    "                                    weight_cat=1.0,\n",
    "                                    weight_num=1.0,\n",
    "                                    backend=\"pytorch\",\n",
    "                                    train_steps=100000,\n",
    "                                    batch_size=100,\n",
    "                                    num_workers=4,\n",
    "                                    seed=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize wandb\n",
    "wandb_project = \"ADULT CounterfactualRL\"\n",
    "wandb.init(project=wandb_project)\n",
    "\n",
    "# fit the explainers\n",
    "explainer = explainer.fit(X=X_train)\n",
    "\n",
    "# close wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.save(\"cfrl_tabular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = CounterfactualRLTabular.load(\"cfrl_tabular\", predictor=predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select some positive examples\n",
    "X_positive = X_test[np.argmax(predictor(X_test), axis=1) == 1]\n",
    "\n",
    "\n",
    "X = X_positive[:100]\n",
    "Y_t = np.array([0])\n",
    "C = [{\"Age\": [0, 20], \"Workclass\": [\"State-gov\", \"?\", \"Local-gov\"]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate counterfactual instances\n",
    "explanation = explainer.explain(X, Y_t, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat labels to the original instances\n",
    "orig = np.concatenate(\n",
    "    [explanation.data['orig']['X'], explanation.data['orig']['class']],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# concat labels to the counterfactual instances\n",
    "cf = np.concatenate(\n",
    "    [explanation.data['cf']['X'], explanation.data['cf']['class']],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# define new feature names and category map by including the label\n",
    "feature_names = adult.feature_names + [\"Label\"]\n",
    "category_map = deepcopy(adult.category_map)\n",
    "category_map.update({feature_names.index(\"Label\"): adult.target_names})\n",
    "\n",
    "# replace label encodings with strings\n",
    "orig_pd = pd.DataFrame(\n",
    "    apply_category_mapping(orig, category_map),\n",
    "    columns=feature_names\n",
    ")\n",
    "\n",
    "cf_pd = pd.DataFrame(\n",
    "    apply_category_mapping(cf, category_map),\n",
    "    columns=feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_pd.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_pd.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate counterfactual instances\n",
    "X = X_positive[2].reshape(1, -1)\n",
    "explanation = explainer.explain(X, Y_t, C, diversity=True, num_samples=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat label column\n",
    "orig = np.concatenate(\n",
    "    [explanation.data['orig']['X'], explanation.data['orig']['class']],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "cf = np.concatenate(\n",
    "    [explanation.data['cf']['X'], explanation.data['cf']['class']],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# transfrom label encodings to string\n",
    "orig_pd = pd.DataFrame(\n",
    "    apply_category_mapping(orig, category_map),\n",
    "    columns=feature_names,\n",
    ")\n",
    "\n",
    "cf_pd = pd.DataFrame(\n",
    "    apply_category_mapping(cf, category_map),\n",
    "    columns=feature_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_pd.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_pd.head(n=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
