{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrated gradients for transformers models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we apply the integrated gradients method to two different sentiment analysis models. The first one is a pretrained sentiment analysis model from the  [transformers](https://github.com/huggingface/transformers) library. The second model is a combination of a pretrained BERT model and a simple feed forward network. The feed forward network is trained on the IMDB dataset using the BERT output embeddings as features. \n",
    "\n",
    "In text classification models, integrated gradients define an attribution value for each word in the input sentence. The attributions are calculated considering the integral of the model  gradients with respect to the word embedding layer along a straight path from a baseline instance $x^\\prime$ to the input instance $x.$ A description of the method can be found [here](https://docs.seldon.io/projects/alibi/en/latest/methods/IntegratedGradients.html). Integrated gradients was originally proposed in Sundararajan et al., [\"Axiomatic Attribution for Deep Networks\"](https://arxiv.org/abs/1703.01365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import BertTokenizerFast, TFBertModel, BertConfig\n",
    "from alibi.explainers import IntegratedGradients\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define some functions needed to process the data. For consistency with other [text examples](https://github.com/SeldonIO/alibi/blob/master/examples/integrated_gradients_imdb.ipynb) in alibi, we will use the IMDB dataset provided by keras. Since the dataset consists of reviews that are already tokenized, we need to decode each sentence and re-convert them into tokens using the BERT tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sentence(x, reverse_index, unk_token: str = '[UNK]'):\n",
    "    \"\"\" \n",
    "    Decodes the tokenized sentences from keras IMDB dataset into plain text.\n",
    "    \"\"\"\n",
    "    # the `-3` offset is due to the special tokens used by keras\n",
    "    # see https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset\n",
    "    return \" \".join([reverse_index.get(i - 3, unk_token) for i in x])\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    \"\"\"\n",
    "    Preprocess the text.\n",
    "    \"\"\"\n",
    "    REPLACE_NO_SPACE = re.compile(\"[.;:,!\\'?\\\"()\\[\\]]\")\n",
    "    REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "    \n",
    "    reviews = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(\" \", line) for line in reviews]\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "def process_sentences(sentence, tokenizer, max_len):\n",
    "    \"\"\"\n",
    "    Tokenize the text sentences.\n",
    "    \"\"\"\n",
    "    # since we are using the model for classification, we need to include special char (i.e, '[CLS]', ''[SEP]')\n",
    "    # check the example here: https://huggingface.co/transformers/v4.4.2/quicktour.html\n",
    "    z = tokenizer(sentence, \n",
    "                  add_special_tokens=True, \n",
    "                  padding='max_length', \n",
    "                  max_length=max_len, \n",
    "                  truncation=True,\n",
    "                  return_token_type_ids=True, \n",
    "                  return_attention_mask = True,  \n",
    "                  return_tensors='np')\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use the tensorflow auto model for sequence classification provided by the [transformers](https://github.com/huggingface/transformers) library. \n",
    "\n",
    "The model is pre-trained on the [Stanford Sentiment Treebank (SST)](https://huggingface.co/datasets/sst) dataset. The Stanford Sentiment Treebank is the first corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language.\n",
    "\n",
    "Each phrase is labelled as either negative, somewhat negative, neutral, somewhat positive or positive. The corpus with all 5 labels is referred to as SST-5 or SST fine-grained. Binary classification experiments on full sentences (negative or somewhat negative vs somewhat positive or positive with neutral sentences discarded) refer to the dataset as SST-2 or SST binary.  In this example, we will use a text classifier pre-trained on the SST-2 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# load model and tokenizer\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "auto_model_bert = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# special tokens string and int representation\n",
    "special_tokens = list(tokenizer.special_tokens_map.values())\n",
    "special_tokens_ids = [tokenizer.encode(stok, add_special_tokens=False)[0] for stok in special_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The automodel output is a custom object containing the output logits. We use a wrapper to transform the output into a tensor and apply a softmax function to the logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoModelWrapper(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, model_bert, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model_bert = model_bert\n",
    "\n",
    "    def call(self, inputs, attention_mask=None):\n",
    "        inputs = tf.cast(inputs, tf.int32)\n",
    "        out = self.model_bert(inputs, attention_mask=attention_mask)\n",
    "        return tf.nn.softmax(out.logits)\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {}\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_model = AutoModelWrapper(auto_model_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate integrated gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "max_len = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we consider some simple sentences such as \"I love you, I like you\", \"I love you, I like you, but I also kind of dislike you\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_test_sample = ['I love you, I like you', \n",
    "                 'I love you, I like you, but I also kind of dislike you',\n",
    "                'Everything is so nice about you']\n",
    "z_test_sample = [z.lower() for z in z_test_sample]\n",
    "z_test_sample = process_sentences(z_test_sample, tokenizer, max_len)\n",
    "x_test_sample = z_test_sample['input_ids'].astype(np.int32)\n",
    "\n",
    "# the values of the kwargs have to be `tf.Tensor`. \n",
    "# see transformers issue #14404: https://github.com/huggingface/transformers/issues/14404\n",
    "kwargs = {k: tf.constant(v) for k,v in z_test_sample.items() if k == 'attention_mask'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The auto model consists of a main BERT layer (layer 0) followed by two dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertMainLayer at 0x7f8a90dccc10>,\n",
       " <keras.layers.core.dense.Dense at 0x7f86f8714cd0>,\n",
       " <keras.layers.core.dense.Dense at 0x7f86f8714f40>,\n",
       " <keras.layers.core.dropout.Dropout at 0x7f86f86bb2b0>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the first transformer's block in the main BERT layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Extracting the first transformer block\n",
    "# bl = auto_model.layers[0].layers[0].transformer.layer[0]\n",
    "bl = auto_model.layers[0].layers[0].embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 50\n",
    "method = \"gausslegendre\"\n",
    "internal_batch_size = 5\n",
    "ig  = IntegratedGradients(auto_model,\n",
    "                          layer=bl,\n",
    "                          n_steps=n_steps, \n",
    "                          method=method,\n",
    "                          internal_batch_size=internal_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "# get predictions\n",
    "predictions = auto_model(x_test_sample, **kwargs).numpy().argmax(axis=1)\n",
    "\n",
    "# get the baselines. Note that the baseline contain special characters and\n",
    "# only the regular tokens are zeroed.\n",
    "baselines = x_test_sample *  np.isin(x_test_sample, special_tokens_ids)\n",
    "\n",
    "# get explanation\n",
    "explanation = ig.explain(x_test_sample, \n",
    "                         forward_kwargs=kwargs,\n",
    "                         baselines=baselines, \n",
    "                         target=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributions shape: (3, 128, 768)\n"
     ]
    }
   ],
   "source": [
    "# Get attributions values from the explanation object\n",
    "attrs = explanation.attributions[0]\n",
    "print('Attributions shape:', attrs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributions shape: (3, 128)\n"
     ]
    }
   ],
   "source": [
    "attrs = attrs.sum(axis=2)\n",
    "print('Attributions shape:', attrs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "x_i = x_test_sample[i]\n",
    "attrs_i = attrs[i]\n",
    "pred = predictions[i]\n",
    "pred_dict = {1: 'Positive review', 0: 'Negative review'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "def  hlstr(string, color='white'):\n",
    "    \"\"\"\n",
    "    Return HTML markup highlighting text with the desired color.\n",
    "    \"\"\"\n",
    "    return f\"<mark style=background-color:{color}>{string} </mark>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize(attrs, cmap='PiYG'):\n",
    "    \"\"\"\n",
    "    Compute hex colors based on the attributions for a single instance.\n",
    "    Uses a diverging colorscale by default and normalizes and scales\n",
    "    the colormap so that colors are consistent with the attributions.\n",
    "    \"\"\"\n",
    "    cmap_bound = np.abs(attrs).max()\n",
    "    norm = mpl.colors.Normalize(vmin=-cmap_bound, vmax=cmap_bound)\n",
    "    cmap = mpl.cm.get_cmap(cmap)\n",
    "    \n",
    "    # now compute hex values of colors\n",
    "    colors = list(map(lambda x: mpl.colors.rgb2hex(cmap(norm(x))), attrs))\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [tokenizer.decode([x_i[i]]) for i in range(len(x_i))]\n",
    "colors = colorize(attrs_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label =  0: Negative review\n"
     ]
    }
   ],
   "source": [
    "print('Predicted label =  {}: {}'.format(pred, pred_dict[pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<mark style=background-color:#f7f7f6>[CLS] </mark><mark style=background-color:#fcdbed>i </mark><mark style=background-color:#e181b5>love </mark><mark style=background-color:#f9d1e8>you </mark><mark style=background-color:#f9eff4>, </mark><mark style=background-color:#f9eef4>i </mark><mark style=background-color:#faeaf2>like </mark><mark style=background-color:#fce5f1>you </mark><mark style=background-color:#f3f6ed>, </mark><mark style=background-color:#c9e8a2>but </mark><mark style=background-color:#f9f1f5>i </mark><mark style=background-color:#f8f2f5>also </mark><mark style=background-color:#edf6e1>kind </mark><mark style=background-color:#e8f5d5>of </mark><mark style=background-color:#276419>dislike </mark><mark style=background-color:#fde0ef>you </mark><mark style=background-color:#f7f7f6>[SEP] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark><mark style=background-color:#f7f7f6>[PAD] </mark>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\".join(list(map(hlstr, words, colors))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis on IMDB with fine-tuned model head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a text classifier fine-tuned on the IMDB dataset. We train a feed forward network which uses the concatenated output embeddings of a pretrained BERT model as input features. The BERT model and the trained ffn are combined to obtain an end-to-end text classifier.\n",
    "\n",
    "It must be noted that training an end-to-end text classifier (i. e. combining the BERT model and the feed forward network before training) instead of training the feed forward network separately is likely to lead to better model performance. However, the latter approach is considerably faster and lighter. We use this approach here since performance optimization is beyond the scope of this notebook and the purpose of this example is to illustrate the integrated gradients method applied to a custom classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(X_train, model, batch_size=50):\n",
    "    args = X_train['input_ids']\n",
    "    # the values of the kwargs have to be `tf.Tensor`. \n",
    "    # see transformers issue #14404: https://github.com/huggingface/transformers/issues/14404\n",
    "    kwargs = {k: tf.constant(v) for k, v in  X_train.items() if k != 'input_ids'}\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((args, kwargs)).batch(batch_size)\n",
    "    \n",
    "    embbedings = []\n",
    "    for X_batch in tqdm(dataset):\n",
    "        args_b, kwargs_b = X_batch\n",
    "        batch_embeddings = model(args_b, **kwargs_b)\n",
    "        \n",
    "        # extract hidden representation for [CLS] token\n",
    "        embedding = batch_embeddings.last_hidden_state[:, 0, :].numpy()\n",
    "        embbedings.append(embedding)\n",
    "        \n",
    "    return np.concatenate(embbedings, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the IMDB dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# remove starting of a sequence int token\n",
    "x_train = [x[1:] for x in x_train]\n",
    "x_test = [x[1:] for x in x_test]\n",
    "\n",
    "# get mappings\n",
    "index = imdb.get_word_index()\n",
    "reverse_index = {value: key for (key, value) in index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract embeddings for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to speed up the training, the BERT embeddings are pre-extracted and used as features by the feed forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# load config\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# load model\n",
    "modelBert = TFBertModel.from_pretrained(\"bert-base-uncased\", config=config)\n",
    "modelBert.trainable = False\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', config=config)\n",
    "\n",
    "# special tokens string and int representation\n",
    "special_tokens = list(tokenizer.special_tokens_map.values())\n",
    "special_tokens_ids = [tokenizer.encode(stok, add_special_tokens=False)[0] for stok in special_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding each sentence in the keras IMDB tokenized dataset to obtain the corresponding plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = [], []\n",
    "\n",
    "# decode training sentences\n",
    "for i in range(len(x_train)):\n",
    "    tr_sentence = decode_sentence(x_train[i], reverse_index, unk_token=tokenizer.unk_token)\n",
    "    X_train.append(tr_sentence)\n",
    "\n",
    "# decode testing sentences\n",
    "for i in range(len(x_test)):\n",
    "    te_sentence = decode_sentence(x_test[i], reverse_index, unk_token=tokenizer.unk_token)\n",
    "    X_test.append(te_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-tokenizing the plain text using the BERT tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize datasets\n",
    "X_train = process_sentences(X_train, tokenizer, max_len)\n",
    "X_test = process_sentences(X_test, tokenizer, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the BERT embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [01:36<00:00,  2.59it/s]\n",
      "100%|██████████| 250/250 [01:37<00:00,  2.56it/s]\n"
     ]
    }
   ],
   "source": [
    "train_embeddings = get_embeddings(X_train, modelBert, batch_size=100)\n",
    "test_embeddings = get_embeddings(X_test, modelBert, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train the model head using the BERT output embeddings as features. The output embeddings are tensors of dimension 100 X 768, where each 768-dimensional vector represents a word in a sentence of 100 words. The embedding vectors are concatenated along the first dimension in order to represents a full review. The model head consists of one dense layer 128 hidden units followed by a 2 units layer with softmax activation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.1\n",
    "hidden_dims = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelOut(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, dropout=0.2, hidden_dims=128):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.hidden_dims = hidden_dims\n",
    "        \n",
    "        self.dense_1 =  tf.keras.layers.Dense(hidden_dims, activation='relu')\n",
    "        self.dropoutl = tf.keras.layers.Dropout(dropout)\n",
    "        self.dense_2 = tf.keras.layers.Dense(2, activation='softmax')\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dropoutl(x, training=training)\n",
    "        x = self.dense_2(x)\n",
    "        return x\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\"dropout\": self.dropout, \"hidden_dims\": self.hidden_dims}\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = ModelOut(dropout=dropout, hidden_dims=hidden_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model. If the model has been already trained, it can be loaded from the checkpoint directory setting `load_model=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False\n",
    "\n",
    "# paper's recommendation\n",
    "learning_rate = 2e-5\n",
    "batch_size = 16\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1559/1563 [============================>.] - ETA: 0s - loss: 0.5727 - accuracy: 0.7118\n",
      "Epoch 00001: saving model to ./model_transformers/training/cp-0001.ckpt\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.5727 - accuracy: 0.7118 - val_loss: 0.4998 - val_accuracy: 0.7671\n",
      "Epoch 2/5\n",
      "1538/1563 [============================>.] - ETA: 0s - loss: 0.4763 - accuracy: 0.7825\n",
      "Epoch 00002: saving model to ./model_transformers/training/cp-0002.ckpt\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4762 - accuracy: 0.7822 - val_loss: 0.4612 - val_accuracy: 0.7856\n",
      "Epoch 3/5\n",
      "1522/1563 [============================>.] - ETA: 0s - loss: 0.4522 - accuracy: 0.7931\n",
      "Epoch 00003: saving model to ./model_transformers/training/cp-0003.ckpt\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4523 - accuracy: 0.7932 - val_loss: 0.4463 - val_accuracy: 0.7924\n",
      "Epoch 4/5\n",
      "1543/1563 [============================>.] - ETA: 0s - loss: 0.4401 - accuracy: 0.7997\n",
      "Epoch 00004: saving model to ./model_transformers/training/cp-0004.ckpt\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4404 - accuracy: 0.7994 - val_loss: 0.4382 - val_accuracy: 0.7980\n",
      "Epoch 5/5\n",
      "1530/1563 [============================>.] - ETA: 0s - loss: 0.4328 - accuracy: 0.8036\n",
      "Epoch 00005: saving model to ./model_transformers/training/cp-0005.ckpt\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4327 - accuracy: 0.8038 - val_loss: 0.4346 - val_accuracy: 0.7995\n"
     ]
    }
   ],
   "source": [
    "filepath = './model_transformers/'  # change to desired save directory\n",
    "\n",
    "model_out.compile(optimizer=Adam(learning_rate), \n",
    "                  loss=SparseCategoricalCrossentropy(from_logits=False), \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "if not load_model:\n",
    "    \n",
    "    checkpoint_path = os.path.join(filepath, \"training/cp-{epoch:04d}.ckpt\")\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "    # Create a callback that saves the model's weights every epoch\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path, \n",
    "        verbose=1, \n",
    "        save_weights_only=True,\n",
    "        save_freq='epoch')\n",
    "\n",
    "    # using the entire testing dataset might result in memory issues when running on GPU\n",
    "    model_out.fit(train_embeddings, y_train, \n",
    "                  validation_data=(test_embeddings, y_test),\n",
    "                  epochs=epochs, \n",
    "                  batch_size=batch_size,\n",
    "                  callbacks=[cp_callback],\n",
    "                  verbose=1)\n",
    "else:\n",
    "    epoch = 3\n",
    "    load_path = os.path.join(filepath, f\"training/cp-{epoch:04d}.ckpt\")\n",
    "    model_out.load_weights(load_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine BERT and feed forward network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we combine the BERT model with the model head to obtain an end-to-end text classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, model_bert, model_out):\n",
    "        super().__init__()\n",
    "        self.model_bert = model_bert\n",
    "        self.model_out = model_out\n",
    "\n",
    "    def call(self, inputs, attention_mask=None, training=False):\n",
    "        out = self.model_bert(inputs, attention_mask=attention_mask, training=training)\n",
    "        out = self.model_out(out.last_hidden_state[:, 0, :], training=training)\n",
    "        return out\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {}\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_classifier = TextClassifier(modelBert, model_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate integrated gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pick the first 10 sentences from the test set as examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_test_sample = [decode_sentence(x_test[i], reverse_index, unk_token=tokenizer.unk_token) for i in range(10)]\n",
    "z_test_sample = process_sentences(z_test_sample, tokenizer, max_len)\n",
    "\n",
    "x_test_sample = z_test_sample['input_ids']\n",
    "# the values of the kwargs have to be `tf.Tensor`. \n",
    "# see transformers issue #14404: https://github.com/huggingface/transformers/issues/14404\n",
    "kwargs = {k:tf.constant(v) for k,v in z_test_sample.items() if k == 'attention_mask'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the attributions with respect to the first embedding layer of the BERT encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bl = text_classifier.layers[0].bert.encoder.layer[0]\n",
    "bl = text_classifier.layers[0].bert.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 50\n",
    "method = \"gausslegendre\"\n",
    "internal_batch_size = 5\n",
    "ig  = IntegratedGradients(text_classifier,\n",
    "                          layer=bl,\n",
    "                          n_steps=n_steps, \n",
    "                          method=method,\n",
    "                          internal_batch_size=internal_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "predictions = text_classifier(x_test_sample, **kwargs).numpy().argmax(axis=1)\n",
    "baselines = x_test_sample * np.isin(x_test_sample, special_tokens_ids)\n",
    "\n",
    "explanation = ig.explain(x_test_sample, \n",
    "                         forward_kwargs=kwargs,\n",
    "                         baselines=baselines, \n",
    "                         target=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributions shape: (10, 128, 768)\n"
     ]
    }
   ],
   "source": [
    "# Get attributions values from the explanation object\n",
    "attrs = explanation.attributions[0]\n",
    "print('Attributions shape:', attrs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributions shape: (10, 128)\n"
     ]
    }
   ],
   "source": [
    "attrs = attrs.sum(axis=2)\n",
    "print('Attributions shape:', attrs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "x_i = x_test_sample[i]\n",
    "attrs_i = attrs[i]\n",
    "pred = predictions[i]\n",
    "pred_dict = {1: 'Positive review', 0: 'Negative review'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "def  hlstr(string, color='white'):\n",
    "    \"\"\"\n",
    "    Return HTML markup highlighting text with the desired color.\n",
    "    \"\"\"\n",
    "    return f\"<mark style=background-color:{color}>{string} </mark>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize(attrs, cmap='PiYG'):\n",
    "    \"\"\"\n",
    "    Compute hex colors based on the attributions for a single instance.\n",
    "    Uses a diverging colorscale by default and normalizes and scales\n",
    "    the colormap so that colors are consistent with the attributions.\n",
    "    \"\"\"\n",
    "    import matplotlib as mpl\n",
    "    cmap_bound = np.abs(attrs).max()\n",
    "    norm = mpl.colors.Normalize(vmin=-cmap_bound, vmax=cmap_bound)\n",
    "    cmap = mpl.cm.get_cmap(cmap)\n",
    "    \n",
    "    # now compute hex values of colors\n",
    "    colors = list(map(lambda x: mpl.colors.rgb2hex(cmap(norm(x))), attrs))\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [tokenizer.decode([x_i[i]]) for i in range(len(x_i))]\n",
    "colors = colorize(attrs_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label =  1: Positive review\n"
     ]
    }
   ],
   "source": [
    "print('Predicted label =  {}: {}'.format(pred, pred_dict[pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<mark style=background-color:#f7f7f6>[CLS] </mark><mark style=background-color:#f8f4f6>this </mark><mark style=background-color:#f9eef4>film </mark><mark style=background-color:#f3f7ef>requires </mark><mark style=background-color:#f6f7f5>a </mark><mark style=background-color:#edf6df>lot </mark><mark style=background-color:#f8f2f5>of </mark><mark style=background-color:#f1f6ea>patience </mark><mark style=background-color:#f8f3f6>because </mark><mark style=background-color:#f7f7f6>it </mark><mark style=background-color:#f4f7f0>focuses </mark><mark style=background-color:#f9f1f5>on </mark><mark style=background-color:#f7f6f7>mood </mark><mark style=background-color:#f9f1f5>and </mark><mark style=background-color:#f9eff4>character </mark><mark style=background-color:#eaf5d9>development </mark><mark style=background-color:#f9f1f5>the </mark><mark style=background-color:#fce4f0>plot </mark><mark style=background-color:#f7f7f7>is </mark><mark style=background-color:#edf6e1>very </mark><mark style=background-color:#f3f6ed>simple </mark><mark style=background-color:#f1f6ea>and </mark><mark style=background-color:#f5f7f2>many </mark><mark style=background-color:#f4f7f0>of </mark><mark style=background-color:#f2f6ec>the </mark><mark style=background-color:#f8f2f5>scenes </mark><mark style=background-color:#f1f6e8>take </mark><mark style=background-color:#f5f7f2>place </mark><mark style=background-color:#eef6e2>on </mark><mark style=background-color:#f3f6ed>the </mark><mark style=background-color:#f5f7f3>same </mark><mark style=background-color:#f9f0f5>set </mark><mark style=background-color:#fcdded>in </mark><mark style=background-color:#e7f5d2>frances </mark><mark style=background-color:#f7f7f6>[UNK] </mark><mark style=background-color:#f8f2f5>the </mark><mark style=background-color:#f7f6f7>sandy </mark><mark style=background-color:#eaf5d9>dennis </mark><mark style=background-color:#e7f5d3>character </mark><mark style=background-color:#fde2f0>apartment </mark><mark style=background-color:#e4f4cd>but </mark><mark style=background-color:#e9f5d6>the </mark><mark style=background-color:#d8efb9>film </mark><mark style=background-color:#faecf3>builds </mark><mark style=background-color:#f7f7f6>to </mark><mark style=background-color:#ebf6dc>a </mark><mark style=background-color:#f5f7f3>disturbing </mark><mark style=background-color:#dbf0bf>climax </mark><mark style=background-color:#f1f6ea>br </mark><mark style=background-color:#eff6e4>br </mark><mark style=background-color:#ecf6de>the </mark><mark style=background-color:#ebf6db>characters </mark><mark style=background-color:#edf6df>create </mark><mark style=background-color:#eff6e4>an </mark><mark style=background-color:#eef6e2>atmosphere </mark><mark style=background-color:#f7f7f6>[UNK] </mark><mark style=background-color:#ebf6db>with </mark><mark style=background-color:#fde0ef>sexual </mark><mark style=background-color:#edf6df>tension </mark><mark style=background-color:#f4f7f0>and </mark><mark style=background-color:#eff6e5>psychological </mark><mark style=background-color:#f7f7f6>[UNK] </mark><mark style=background-color:#ebf6dc>it </mark><mark style=background-color:#eff6e5>' </mark><mark style=background-color:#f1f6ea>s </mark><mark style=background-color:#eff6e4>very </mark><mark style=background-color:#a7d672>interesting </mark><mark style=background-color:#eff6e4>that </mark><mark style=background-color:#f5f7f2>robert </mark><mark style=background-color:#f7f6f7>alt </mark><mark style=background-color:#f9f0f5>##man </mark><mark style=background-color:#f7f7f6>directed </mark><mark style=background-color:#f0f6e7>this </mark><mark style=background-color:#f1f6ea>considering </mark><mark style=background-color:#edf6df>the </mark><mark style=background-color:#eef6e2>style </mark><mark style=background-color:#f1f6e8>and </mark><mark style=background-color:#f6f7f5>structure </mark><mark style=background-color:#f5f7f2>of </mark><mark style=background-color:#ecf6de>his </mark><mark style=background-color:#e9f5d6>other </mark><mark style=background-color:#f8f4f6>films </mark><mark style=background-color:#fce3f0>still </mark><mark style=background-color:#acd977>the </mark><mark style=background-color:#eff6e4>trademark </mark><mark style=background-color:#f6f7f5>alt </mark><mark style=background-color:#dff2c4>##man </mark><mark style=background-color:#e99cc8>audio </mark><mark style=background-color:#b2dd7f>style </mark><mark style=background-color:#eff6e5>is </mark><mark style=background-color:#6bac34>evident </mark><mark style=background-color:#f2f6ec>here </mark><mark style=background-color:#f5f7f2>and </mark><mark style=background-color:#f2f6ec>there </mark><mark style=background-color:#f7f7f7>i </mark><mark style=background-color:#d0ecad>think </mark><mark style=background-color:#8ac34f>what </mark><mark style=background-color:#c0e593>really </mark><mark style=background-color:#f1f6ea>makes </mark><mark style=background-color:#e2f3ca>this </mark><mark style=background-color:#fce3f0>film </mark><mark style=background-color:#c9e8a2>work </mark><mark style=background-color:#e7f5d2>is </mark><mark style=background-color:#ecf6de>the </mark><mark style=background-color:#dff2c4>brilliant </mark><mark style=background-color:#fcdbed>performance </mark><mark style=background-color:#f9eef4>by </mark><mark style=background-color:#eff6e5>sandy </mark><mark style=background-color:#cbe9a4>dennis </mark><mark style=background-color:#dbf0bf>it </mark><mark style=background-color:#f3f7ef>' </mark><mark style=background-color:#e7f5d3>s </mark><mark style=background-color:#f8f4f6>definitely </mark><mark style=background-color:#dbf0bf>one </mark><mark style=background-color:#f0f6e7>of </mark><mark style=background-color:#b0dc7d>her </mark><mark style=background-color:#fce5f1>darker </mark><mark style=background-color:#f5f7f2>characters </mark><mark style=background-color:#faebf3>but </mark><mark style=background-color:#c6e79c>she </mark><mark style=background-color:#fcdbed>plays </mark><mark style=background-color:#a1d26a>it </mark><mark style=background-color:#d2ecb0>so </mark><mark style=background-color:#a1d26a>perfectly </mark><mark style=background-color:#69aa33>and </mark><mark style=background-color:#faeaf2>convincing </mark><mark style=background-color:#8e0152>##ly </mark><mark style=background-color:#f7f7f6>[SEP] </mark>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\".join(list(map(hlstr, words, colors))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alibi",
   "language": "python",
   "name": "alibi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
