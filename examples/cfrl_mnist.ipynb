{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counterfactual with Reinforcement Learning (CfRL) on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is described in [Model-agnostic and Scalable Counterfactual Explanations via Reinforcement Learning](https://arxiv.org/abs/2106.02597) and can generatae counterfactual instances for any black-box model. The usual optimization procedure is transformed into a learnable process allowing to generate batches of counterfactual instances in a single forward pass even for high dimensional data. The training pipeline is model-agnostic and relies only on prediction feedback by querying the black-box model. Furthermore, the method allows target and feature conditioning. \n",
    "\n",
    "CfRL uses [Deep Deterministic Policy Gradient (DDPG)](https://arxiv.org/abs/1509.02971) by interleaving a state-action function approximatr called critic, with a learning an approximator called actor to predict the optimal action. The method assumes that the critic is differentiable with respect to the action argument, thus allowing to optimize the actor's parameters efficiently through gradient-based methods.\n",
    "\n",
    "The DDPG algorithm requires two separate networks, an actor $\\mu$ and a critic $Q$. Given the encoded representation of the input instance $z = enc(x)$, the model prediction $y_M$, the target prediction\n",
    "$y_T$ and the conditioning vector $c$, the actor outputs the counterfactual’s latent representation $z_{CF} = \\mu(z, y_M, y_T, c)$. The decoder then projects the embedding $z_{CF}$ back to the original input space,\n",
    "followed by optional post-processing.\n",
    "\n",
    "The training step consists of simultaneously optimizing the actor and critic networks. The critic regresses on the reward $R$ determined by the model prediction, while the actor maximizes the critic’s output for the given instance through $L_{max}$. The actor also minimizes two objectives to encourage the generation of sparse, indistribution counterfactuals. The sparsity loss $L_{sparsity}$ operates on the decoded counterfactual $x_{CF}$ and combines the $L_1$ loss over the standardized numerical features and the $L_0$ loss over the categorical ones. The consistency loss $L_{consist}$ aims to encode the counterfactual $x_{CF}$ back to the same latent representation where it was decoded from and helps to produce in-distribution counterfactual instances.\n",
    "\n",
    "Formally, the actor's loss can be written as:\n",
    "$L_{actor} = L_{max} + \\lambda_{1}L_{sparsity} + \\lambda_{2}L_{consistency}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Dict\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from alibi.models.tensorflow.autoencoder import AE\n",
    "from alibi.models.tensorflow.actor_critic import Actor, Critic\n",
    "from alibi.models.tensorflow.cfrl_models import MNISTEncoder, MNISTDecoder, MNISTClassifier\n",
    "from alibi.explainers.cfrl_base import CounterfactualRLBase, ExperienceCallback, TrainingCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants.\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1024\n",
    "\n",
    "# Load MNIST dataset.\n",
    "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Expand dimensions and normalize.\n",
    "X_train = np.expand_dims(X_train, axis=-1).astype(np.float) / 255.\n",
    "X_test = np.expand_dims(X_test, axis=-1).astype(np.float) / 255.\n",
    "\n",
    "# Define trainset.\n",
    "trainset_classifier = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "trainset_classifier = trainset_classifier.shuffle(buffer_size=BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "# Define testset.\n",
    "testset_classifier = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n",
    "testset_classifier = testset_classifier.shuffle(buffer_size=BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and train CNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes.\n",
    "NUM_CLASSES = 10\n",
    "EPOCHS = 5\n",
    "\n",
    "# Define classifier path and create dir if it doesn't exist.\n",
    "classifier_path = os.path.join(\"tensorflow\", \"MNIST_classifier\")\n",
    "if not os.path.exists(classifier_path):\n",
    "    os.makedirs(classifier_path)\n",
    "\n",
    "# Construct classifier. This is the classifier used in the paper experiments.\n",
    "classifier = MNISTClassifier(output_dim=NUM_CLASSES)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Complile the model.\n",
    "classifier.compile(optimizer=optimizer, \n",
    "                   loss=loss,\n",
    "                   metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "\n",
    "if len(os.listdir(classifier_path)) == 0:\n",
    "    # Fit and save the classifier.\n",
    "    classifier.fit(trainset_classifier, epochs=EPOCHS)\n",
    "    classifier.save(classifier_path)\n",
    "else:\n",
    "    # Load the classifier if already fitted.\n",
    "    classifier = keras.models.load_model(classifier_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the classifier\n",
    "loss, accuracy = classifier.evaluate(testset_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the predictor (black-box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've trained the CNN classifier, we can define the black-box model. Note that the output of the black-box is a distribution which can be either a soft-label distirbution (probabilities/logits for each class) or a hard-label distribution (one-hot encoding). Internally, CfRL takes the argmax. Morover the output **DOES NOT HAVE TO BE DIFFERENTIABLE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predictor function (black-box) used to train the CfRL\n",
    "def predictor(X: np.ndarray):\n",
    "    Y = classifier(X).numpy()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and train autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of directly modelling the perturbation vector in the potentially high-dimensional input space, we first train an autoencoder. The weights of the encoder are frozen and the actor applies the\n",
    "counterfactual perturbations in the latent space of the encoder. The pre-trained decoder maps the counterfactual embedding back to the input feature space. \n",
    "\n",
    "The autoencoder follows a standard design. The model is composed from two submodules, the encoder and the decoder. The forward pass consists of passing the input to the encoder, obtain the input embedding and pass the embedding through the decoder.\n",
    "\n",
    "```python\n",
    "class AE(keras.Model):\n",
    "    def __init__(self, encoder: keras.Model, decoder: keras.Model, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, x: tf.Tensor, **kwargs):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define autoencoder trainset.\n",
    "trainset_ae = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "trainset_ae = trainset_ae.map(lambda x: (x, x))\n",
    "trainset_ae = trainset_ae.shuffle(buffer_size=BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "# Define autoencode testset.\n",
    "testset_ae = tf.data.Dataset.from_tensor_slices(X_test)\n",
    "testset_ae = testset_ae.map(lambda x: (x, x))\n",
    "testset_ae = testset_ae.shuffle(buffer_size=BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define autoencoder path and create dir if it doesn't exist.\n",
    "ae_path = os.path.join(\"tensorflow\", \"MNIST_autoencoder\")\n",
    "if not os.path.exists(ae_path):\n",
    "    os.makedirs(ae_path)\n",
    "\n",
    "# Define latent dimension.\n",
    "LATENT_DIM = 64\n",
    "EPOCHS = 50\n",
    "    \n",
    "# Define autoencoder.\n",
    "ae = AE(encoder=MNISTEncoder(latent_dim=LATENT_DIM),\n",
    "        decoder=MNISTDecoder())\n",
    "\n",
    "# Define optimizer and loss function.\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "# Compile autoencoder.\n",
    "ae.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "if len(os.listdir(ae_path)) == 0:\n",
    "    # Fit and save autoencoder.\n",
    "    ae.fit(trainset_ae, epochs=EPOCHS)\n",
    "    ae.save(ae_path)\n",
    "else:\n",
    "    # Load the model.\n",
    "    ae = keras.models.load_model(ae_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of samples to be displayed\n",
    "NUM_SAMPLES = 5\n",
    "\n",
    "# Get some random samples from test\n",
    "np.random.seed(0)\n",
    "indices = np.random.choice(X_test.shape[0], NUM_SAMPLES)\n",
    "inputs = [X_test[i].reshape(1, 28, 28, 1) for i in indices]\n",
    "inputs = np.concatenate(inputs, axis=0)\n",
    "\n",
    "# Pass samples through the autoencoder\n",
    "inputs_hat = ae(inputs).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot inputs and reconstructions.\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "fig, ax = plt.subplots(2, NUM_SAMPLES, figsize=(25, 10))\n",
    "\n",
    "for i in range(NUM_SAMPLES):\n",
    "    ax[0][i].imshow(inputs[i])\n",
    "    ax[1][i].imshow(inputs_hat[i])\n",
    "    \n",
    "text1 = ax[0][0].set_ylabel(\"x\")\n",
    "text2 = ax[1][0].set_ylabel(\"x_hat\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counterfactual with Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "COEFF_SPARSITY = 7.5               # sparisty coefficient\n",
    "COEFF_CONSISTENCY = 0              # consisteny coefficient -> No consistency.\n",
    "TRAIN_STEPS = 25000                # number of training steps -> increase this to 150.000 or even further\n",
    "BATCH_SIZE = 100                   # batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define and fit the explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define explainer.\n",
    "explainer = CounterfactualRLBase(predictor=predictor,\n",
    "                                 encoder=ae.encoder,\n",
    "                                 decoder=ae.decoder,\n",
    "                                 latent_dim=LATENT_DIM,\n",
    "                                 coeff_sparsity=COEFF_SPARSITY,\n",
    "                                 coeff_consistency=COEFF_CONSISTENCY,\n",
    "                                 train_steps=TRAIN_STEPS,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 backend=\"tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the explainer\n",
    "exaplainer = explainer.fit(X=X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate counterfactuals for some test instances.\n",
    "explanation = explainer.explain(X_test[0:200], Y_t=np.array([1]), batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, NUM_SAMPLES, figsize=(25, 10))\n",
    "\n",
    "for i in range(NUM_SAMPLES):\n",
    "    ax[0][i].imshow(explanation.data['orig']['X'][i])\n",
    "    ax[1][i].imshow(explanation.data['cf']['X'][i])\n",
    "    \n",
    "    ax[0][i].set_xlabel(\"Label: \" + str(explanation.data['orig']['class'][i]))\n",
    "    ax[1][i].set_xlabel(\"Label: \" + str(explanation.data['cf']['class'][i]))\n",
    "    \n",
    "\n",
    "text1 = ax[0][0].set_ylabel(\"X\")\n",
    "text2 = ax[1][0].set_ylabel(\"X_hat\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging is clearly important when dealing with deep learning models. Thus, we provide two interfaces to write custom callbacks for logging purposes:\n",
    "\n",
    "* an `ExperienceCallback`, called after each collected experince step;\n",
    "\n",
    "```python\n",
    "class ExperienceCallback:\n",
    "    def __call__(self,\n",
    "                 step: int,\n",
    "                 model: CounterfactualRLBase,\n",
    "                 sample: Dict[str, np.ndarray]) -> None:\n",
    "        \"\"\"\n",
    "        Experience call-back applied after gather an experience.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        step\n",
    "            Current experience step.\n",
    "        model\n",
    "            CounterfactualRLBase explainer.\n",
    "        sample\n",
    "            Dictionary of sample gathered in an experience. This includes dataset inputs and intermediate\n",
    "            results obtained during an experience.\n",
    "        \"\"\"\n",
    "        pass\n",
    "```\n",
    "\n",
    "* a `TrainingCallback`, called after each training step;\n",
    "\n",
    "```python\n",
    "class TrainingCallback:\n",
    "    def __call__(self,\n",
    "                 step: int,\n",
    "                 update: int,\n",
    "                 model: CounterfactualRLBase,\n",
    "                 sample: Dict[str, np.ndarray],\n",
    "                 losses: Dict[str, float]) -> None:\n",
    "        \"\"\"\n",
    "        Training call-back applied after every training step.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        step\n",
    "            Current experience step.\n",
    "        update\n",
    "            Current update. The ration between the number experience steps and the number of training \n",
    "            updates is bound to 1.\n",
    "        model\n",
    "            CounterfactualRLBase explainer.\n",
    "        sample\n",
    "            Dictionary of samples used for an update. This is sampled from the replay buffer.\n",
    "        losses\n",
    "            Dictionary of losses.\n",
    "        \"\"\"\n",
    "        pass\n",
    "```\n",
    "\n",
    "In the following cells we provide some example to log in **Weights and Biases**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging reward callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardCallback(ExperienceCallback):\n",
    "    def __call__(self,\n",
    "                 step: int, \n",
    "                 model: CounterfactualRLBase, \n",
    "                 sample: Dict[str, np.ndarray]):\n",
    "        if step % 100 != 0:\n",
    "            return\n",
    "        \n",
    "        # Get the counterfactual and target.\n",
    "        X_cf = sample[\"X_cf\"]\n",
    "        Y_t = sample[\"Y_t\"]\n",
    "        \n",
    "        # Get prediction label.\n",
    "        Y_m_cf = predictor(X_cf)\n",
    "        \n",
    "        # Compute reward\n",
    "        reward = np.mean(model.params[\"reward_func\"](Y_m_cf, Y_t))\n",
    "        wandb.log({\"reward\": reward})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging images callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesCallback(ExperienceCallback):\n",
    "    def __call__(self,\n",
    "                 step:int,\n",
    "                 model: CounterfactualRLBase,\n",
    "                 sample: Dict[str, np.ndarray]):\n",
    "        # Log every 100 steps\n",
    "        if step % 100 != 0:\n",
    "            return\n",
    "        \n",
    "        # Defie number of samples to be displayed.\n",
    "        NUM_SAMPLES = 10\n",
    "        \n",
    "        X = sample[\"X\"][:NUM_SAMPLES]        # input instance\n",
    "        X_cf = sample[\"X_cf\"][:NUM_SAMPLES]  # counterfactual\n",
    "        diff = np.abs(X - X_cf)              # differences\n",
    "        \n",
    "        Y_m = sample[\"Y_m\"][:NUM_SAMPLES].astype(int)   # input labels\n",
    "        Y_t = sample[\"Y_t\"][:NUM_SAMPLES].astype(int)   # target labels\n",
    "        Y_m_cf = predictor(X_cf).astype(int)            # counterfactual labels\n",
    "        \n",
    "        # Concatentate images,\n",
    "        X = np.concatenate(X, axis=1)\n",
    "        X_cf = np.concatenate(X_cf, axis=1)\n",
    "        diff = np.concatenate(diff, axis=1)\n",
    "        \n",
    "        # Construct full image.\n",
    "        img = np.concatenate([X, X_cf, diff], axis=0)\n",
    "            \n",
    "        # Construct caption.\n",
    "        caption = \"\"\n",
    "        caption += \"Input:\\t%s\\n\" % str(list(np.argmax(Y_m, axis=1)))\n",
    "        caption += \"Target:\\t%s\\n\" % str(list(np.argmax(Y_t, axis=1)))\n",
    "        caption += \"Predicted:\\t%s\\n\" % str(list(np.argmax(Y_m_cf, axis=1)))\n",
    "        \n",
    "        # Log image.\n",
    "        wandb.log({\"samples\": wandb.Image(img, caption=caption)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging losses callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossCallback(TrainingCallback):\n",
    "    def __call__(self,\n",
    "                 step: int, \n",
    "                 update: int, \n",
    "                 model: CounterfactualRLBase,\n",
    "                 sample: Dict[str, np.ndarray],\n",
    "                 losses: Dict[str, float]):\n",
    "        # Log evary 100 updates.\n",
    "        if (step + update) % 100 == 0:\n",
    "            wandb.log(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the callbacks, we can define a new explainer that will include logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define explainer.\n",
    "explainer = CounterfactualRLBase(predictor=predictor,\n",
    "                                 encoder=ae.encoder,\n",
    "                                 decoder=ae.decoder,\n",
    "                                 latent_dim=LATENT_DIM,\n",
    "                                 coeff_sparsity=COEFF_SPARSITY,\n",
    "                                 coeff_consistency=COEFF_CONSISTENCY,\n",
    "                                 train_steps=TRAIN_STEPS,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 experience_callbacks=[RewardCallback(), ImagesCallback()], # <--- here\n",
    "                                 train_callbacks=[LossCallback()],                          # <--- here\n",
    "                                 backend=\"tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialize wandb.\n",
    "wandb_project = \"MNIST Counterfactual with Reinforcement Learning\"\n",
    "wandb.init(project=wandb_project)\n",
    "\n",
    "# Fit the explainer.\n",
    "explainer.fit(X=X_train)\n",
    "\n",
    "# Close wandb.\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
