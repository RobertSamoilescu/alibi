{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import wandb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Dict\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from alibi.models.tensorflow.autoencoder import AE\n",
    "from alibi.models.tensorflow.actor_critic import Actor, Critic\n",
    "from alibi.models.tensorflow.cfrl_models import MNISTEncoder, MNISTDecoder, MNISTClassifier\n",
    "from alibi.explainers.cfrl_base import CounterfactualRLBase, ExperienceCallback, TrainingCallback\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mnist dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# define trainset\n",
    "trainset_classifier = tf.data.Dataset.from_tensor_slices((\n",
    "    np.expand_dims(x_train, axis=-1).astype(np.float) / 255., y_train))\n",
    "trainset_classifier = trainset_classifier.shuffle(buffer_size=1024).batch(BATCH_SIZE)\n",
    "\n",
    "# define testset\n",
    "testset_classifier = tf.data.Dataset.from_tensor_slices(\n",
    "    (np.expand_dims(x_test, axis=-1).astype(np.float) / 255., y_test))\n",
    "testset_classifier = testset_classifier.shuffle(buffer_size=1024).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of classes\n",
    "num_classes = 10\n",
    "\n",
    "# define classifier\n",
    "classifier_path = \"tensorflow/classifier/classifier_mnist.tf\"\n",
    "classifier = MNISTClassifier(output_dim=num_classes)\n",
    "\n",
    "# define optimizer and loss\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# fit and compile\n",
    "classifier.compile(optimizer=optimizer, \n",
    "                   loss=loss,\n",
    "                   metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "\n",
    "if len(glob.glob(classifier_path + \"*\")) == 0:\n",
    "    # fit and save the classifier\n",
    "    classifier.fit(trainset_classifier, epochs=5)\n",
    "    classifier.save_weights(classifier_path)\n",
    "else:\n",
    "    # load the classifier\n",
    "    classifier.load_weights(classifier_path).expect_partial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the classifier\n",
    "classifier.evaluate(testset_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and train AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define trainset\n",
    "trainset_ae = tf.data.Dataset.from_tensor_slices(\n",
    "    np.expand_dims(x_train, axis=-1).astype(np.float) / 255.\n",
    ")\n",
    "trainset_ae = trainset_ae.map(lambda x: (x, x))\n",
    "trainset_ae = trainset_ae.shuffle(buffer_size=1024).batch(BATCH_SIZE)\n",
    "\n",
    "# define testset\n",
    "testset_ae = tf.data.Dataset.from_tensor_slices(\n",
    "    np.expand_dims(x_test, axis=-1).astype(np.float) / 255.\n",
    ")\n",
    "testset_ae = testset_ae.map(lambda x: (x, x))\n",
    "testset_ae = testset_ae.shuffle(buffer_size=1024).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define autoencoder\n",
    "ae_path = \"tensorflow/autoencoder/autoencoder_mnist.tf\"\n",
    "ae = AE(encoder=MNISTEncoder(latent_dim=latent_dim),\n",
    "        decoder=MNISTDecoder())\n",
    "\n",
    "# define optimizer and loss\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "# compile autoencoder\n",
    "ae.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "if len(glob.glob(ae_path + \"*\")) == 0:\n",
    "    # fit and save autoencoder\n",
    "    ae.fit(trainset_ae, epochs=50)\n",
    "    ae.save_weights(ae_path)\n",
    "else:\n",
    "    # load the model\n",
    "    ae.load_weights(ae_path).expect_partial()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random samples from test\n",
    "num_samples = 5\n",
    "np.random.seed(0)\n",
    "indices = np.random.choice(x_test.shape[0], num_samples)\n",
    "inputs = [x_test[i].reshape(1, 28, 28, 1) for i in indices]\n",
    "inputs = np.concatenate(inputs, axis=0) / 255.\n",
    "\n",
    "# pass samples through the autoencoder\n",
    "inputs_hat = ae(inputs).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot inputs and reconstructions\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "fig, ax = plt.subplots(2, num_samples, figsize=(25, 10))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    ax[0][i].imshow(inputs[i])\n",
    "    ax[1][i].imshow(inputs_hat[i])\n",
    "    \n",
    "    if i == 0:\n",
    "        ax[0][i].set_ylabel(\"x\")\n",
    "        ax[1][i].set_ylabel(\"x_hat\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counterfactual RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prediction function\n",
    "def predict_func(X: np.ndarray):\n",
    "    y = tf.argmax(classifier(X), axis=1).numpy()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardCallback(ExperienceCallback):\n",
    "    def __call__(self,\n",
    "                 step: int, \n",
    "                 model: CounterfactualRLBase, \n",
    "                 sample: Dict[str, np.ndarray]):\n",
    "        if step % 100 != 0:\n",
    "            return\n",
    "        \n",
    "        # get the counterfactual and target\n",
    "        x_cf = sample[\"x_cf\"]\n",
    "        y_t = sample[\"y_t\"]\n",
    "        \n",
    "        # get prediction label\n",
    "        y_m_cf = predict_func(x_cf)\n",
    "        \n",
    "        # compute reward\n",
    "        reward = np.mean(model.params[\"reward_func\"](y_m_cf, y_t))\n",
    "        wandb.log({\"reward\": reward})\n",
    "        \n",
    "class DisplayImgsCallback(ExperienceCallback):\n",
    "    def __call__(self,\n",
    "                 step:int,\n",
    "                 model: CounterfactualRLBase,\n",
    "                 sample: Dict[str, np.ndarray]):\n",
    "        if step % 100 != 0:\n",
    "            return\n",
    "        \n",
    "        # defie number of samples to be displayed\n",
    "        num_sample = 5\n",
    "        \n",
    "        x = sample[\"x\"][:num_samples]        # input instance\n",
    "        x_cf = sample[\"x_cf\"][:num_samples]  # counterfactual\n",
    "        diff = np.abs(x - x_cf)              # differences\n",
    "        \n",
    "        y_m = sample[\"y_m\"][:num_samples].astype(int)   # input labels\n",
    "        y_t = sample[\"y_t\"][:num_samples].astype(int)   # target labels\n",
    "        y_m_cf = predict_func(x_cf).astype(int)         # counterfactual labels\n",
    "        \n",
    "        # concatentate images\n",
    "        x = np.concatenate(x, axis=1)\n",
    "        x_cf = np.concatenate(x_cf, axis=1)\n",
    "        diff = np.concatenate(diff, axis=1)\n",
    "        \n",
    "        # construct image\n",
    "        img = np.concatenate([x, x_cf, diff], axis=0)\n",
    "            \n",
    "        # construct caption\n",
    "        caption = \"\"\n",
    "        caption += \"Input:\\t%s\\n\" % str(list(y_m))\n",
    "        caption += \"Target:\\t%s\\n\" % str(list(y_t))\n",
    "        caption += \"Predicted:\\t%s\\n\" % str(list(y_m_cf))\n",
    "        \n",
    "        # log image\n",
    "        wandb.log({\"samples\": wandb.Image(img, caption=caption)})\n",
    "\n",
    "\n",
    "class DisplayLossCallback(TrainingCallback):\n",
    "    def __call__(self,\n",
    "                 step: int, \n",
    "                 update: int, \n",
    "                 model: CounterfactualRLBase,\n",
    "                 sample: Dict[str, np.ndarray],\n",
    "                 losses: Dict[str, float]):\n",
    "        # log training losses\n",
    "        if (step + update) % 100 == 0:\n",
    "            wandb.log(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ddpg\n",
    "explainer = CounterfactualRLBase(predict_func=predict_func,\n",
    "                                 ae=ae,\n",
    "                                 latent_dim=latent_dim,\n",
    "                                 coeff_sparsity=7.5,\n",
    "                                 coeff_consistency=0,\n",
    "                                 num_classes=10,\n",
    "                                 backend=\"tensorflow\",\n",
    "                                 experience_callbacks=[RewardCallback(), DisplayImgsCallback()],\n",
    "                                 train_callbacks=[DisplayLossCallback()],\n",
    "                                 train_steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# initialize wandb\n",
    "wandb_project = \"MNIST CounterfactualRL\"\n",
    "wandb.init(project=wandb_project)\n",
    "\n",
    "# add channel and normalize\n",
    "x = np.expand_dims(x_train, axis=-1) / 255.\n",
    "\n",
    "# fit the explainer\n",
    "explainer.fit(x=x)\n",
    "\n",
    "# close wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 25\n",
    "x_cf = explainer.explain(x[i:i+1], y_t=np.array([0]))\n",
    "\n",
    "plt.imshow(x[i])\n",
    "plt.title(f\"Label: {predict_func(x[i:i+1]).item()}\")\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(x_cf[0])\n",
    "plt.title(f\"Label: {predict_func(x_cf).item()}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
