{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anchor explanations for movie sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will explain why a certain sentence is classified by a logistic regression as having negative or positive sentiment. The logistic regression is trained on negative and positive movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robert/anaconda3/envs/alibi/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "import string\n",
    "from alibi.explainers import AnchorText\n",
    "from alibi.datasets import fetch_movie_sentiment\n",
    "from alibi.utils.download import spacy_model\n",
    "from alibi.utils.lang_model import DistilbertBaseUncased, BertBaseUncased, RobertaBase\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load movie review dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fetch_movie_sentiment` function returns a `Bunch` object containing the features, the targets and the target names for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = fetch_movie_sentiment()\n",
    "movies.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = movies.data\n",
    "labels = movies.target\n",
    "target_names = movies.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define shuffled training, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, train_labels, test_labels = train_test_split(data, labels, test_size=.2, random_state=42)\n",
    "train, val, train_labels, val_labels = train_test_split(train, train_labels, test_size=.1, random_state=42)\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "val_labels = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply CountVectorizer to training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=1)\n",
    "vectorizer.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "clf = LogisticRegression(solver='liblinear')\n",
    "clf.fit(vectorizer.transform(train), train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fn = lambda x: clf.predict(vectorizer.transform(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.980\n",
      "Validation accuracy: 0.754\n",
      "Test accuracy: 0.759\n"
     ]
    }
   ],
   "source": [
    "preds_train = predict_fn(train)\n",
    "preds_val = predict_fn(val)\n",
    "preds_test = predict_fn(test)\n",
    "print('Train accuracy: %.3f' % accuracy_score(train_labels, preds_train))\n",
    "print('Validation accuracy: %.3f' % accuracy_score(val_labels, preds_val))\n",
    "print('Test accuracy: %.3f' % accuracy_score(test_labels, preds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load spaCy model\n",
    "\n",
    "English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Assigns word vectors, context-specific token vectors, POS tags, dependency parse and named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'en_core_web_md'\n",
    "spacy_model(model=model)\n",
    "nlp = spacy.load(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance to be explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Text: a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification .\n",
      "* Prediction: negative\n"
     ]
    }
   ],
   "source": [
    "class_names = movies.target_names\n",
    "\n",
    "# select instance to be explained\n",
    "text = data[4]\n",
    "print(\"* Text: %s\" % text)\n",
    "\n",
    "# compute class prediction\n",
    "pred = class_names[predict_fn([text])[0]]\n",
    "alternative =  class_names[1 - predict_fn([text])[0]]\n",
    "print(\"* Prediction: %s\" % pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize anchor text explainer with `unknown` sampling\n",
    "\n",
    "* `sampling='unkonw'` means we will perturb examples by replacing words with UNKs. Let us now take a look at the anchor. The word 'exercise' basically guarantees a negative prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = AnchorText(\n",
    "    predictor=predict_fn, \n",
    "    sampling_method='unknown',\n",
    "    nlp=nlp,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = explainer.explain(text, threshold=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: flashy\n",
      "Precision: 0.99\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "a UNK flashy UNK UNK opaque and emotionally vapid exercise in style UNK mystification .\n",
      "a UNK flashy UNK UNK UNK and emotionally UNK exercise UNK UNK and UNK UNK\n",
      "a UNK flashy UNK narratively opaque UNK UNK UNK exercise in style and UNK UNK\n",
      "UNK visually flashy UNK narratively UNK and emotionally UNK UNK UNK UNK UNK mystification .\n",
      "UNK UNK flashy UNK UNK opaque and emotionally UNK UNK in UNK and UNK .\n",
      "a visually flashy but UNK UNK and UNK UNK UNK in style UNK mystification .\n",
      "a visually flashy but UNK opaque UNK emotionally vapid UNK in UNK and mystification .\n",
      "a UNK flashy but narratively UNK UNK emotionally vapid exercise in style UNK mystification UNK\n",
      "a UNK flashy but narratively opaque UNK emotionally vapid exercise in style and mystification .\n",
      "a visually flashy UNK UNK opaque UNK UNK UNK exercise in UNK UNK UNK .\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "UNK UNK flashy but narratively UNK and UNK UNK UNK in style and UNK UNK\n"
     ]
    }
   ],
   "source": [
    "print('Anchor: %s' % (' AND '.join(explanation.anchor)))\n",
    "print('Precision: %.2f' % explanation.precision)\n",
    "print('\\nExamples where anchor applies and model predicts %s:' % pred)\n",
    "print('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_true']]))\n",
    "print('\\nExamples where anchor applies and model predicts %s:' % alternative)\n",
    "print('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_false']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize anchor text explainer with word `similarity` sampling\n",
    "\n",
    "Let's try this with another perturbation distribution, namely one that replaces words by similar words instead of UNKs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = AnchorText(\n",
    "    predictor=predict_fn, \n",
    "    sampling_method='similarity',     # replace masked words by simialar words\n",
    "    nlp=nlp,                          # spacy object\n",
    "    sample_proba=0.5,                 # probability of a word to be masked and replace by as similar word\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = explainer.explain(text, threshold=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The anchor now shows that we need more to guarantee the negative prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Anchor: %s' % (' AND '.join(explanation.anchor)))\n",
    "print('Precision: %.2f' % explanation.precision)\n",
    "print('\\nExamples where anchor applies and model predicts %s:' % pred)\n",
    "print('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_true']]))\n",
    "print('\\nExamples where anchor applies and model predicts %s:' % alternative)\n",
    "print('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_false']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the token perturbation distribution sample words that are more similar to the ground truth word via the `top_n` argument. Smaller values (default=100) should result in sentences that are more coherent and thus more in the distribution of natural language which could influence the returned anchor. By setting the `use_proba` to True, the sampling distribution for perturbed tokens is proportional to the similarity score between the possible perturbations and the original word. We can also put more weight on similar words via the `temperature` argument. Lower values of `temperature` increase the sampling weight of more similar words. The following example will perturb tokens in the original sentence with probability equal to `sample_proba`. The sampling distribution for the perturbed tokens is proportional to the similarity score between the ground truth word and each of the `top_n` words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = AnchorText(\n",
    "    predictor=predict_fn, \n",
    "    sampling_method='similarity',   # replace masked words by simialar words\n",
    "    nlp=nlp,                        # spacy object\n",
    "    use_proba=True,                 # sample according to the similiary distribution\n",
    "    sample_proba=0.5,               # probability of a word to be masked and replace by as similar word\n",
    "    top_n=20,                       # consider only top 20 words most similar words\n",
    "    temperature=0.2                 # higher temperature implies more randomness when sampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = explainer.explain(text, threshold=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Anchor: %s' % (' AND '.join(explanation.anchor)))\n",
    "print('Precision: %.2f' % explanation.precision)\n",
    "print('\\nExamples where anchor applies and model predicts %s:' % pred)\n",
    "print('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_true']]))\n",
    "print('\\nExamples where anchor applies and model predicts %s:' % alternative)\n",
    "print('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_false']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForMaskedLM: ['activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# initialize model (any of the following 3 is supported)\n",
    "# language_model = RobertaBase()\n",
    "# language_model = BertBaseUncased()\n",
    "language_model = DistilbertBaseUncased()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize anchor text explainer with `language_model` sampling (`parallel` filling method )\n",
    "\n",
    "* `sampling_method='language_model'` means that the words will be sampled according to the output distribution predicted by the language model\n",
    "\n",
    "* `filling_method='parallel'` means the only one forward pass is performed. The words are the sampled independently of one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize explainer\n",
    "explainer = AnchorText(\n",
    "    predictor=predict_fn,\n",
    "    sampling_method=\"language_model\",    # use language model to predict the masked words\n",
    "    language_model=language_model,       # language model to be used\n",
    "    filling_method=\"parallel\",           # just one pass through the transformer\n",
    "    sample_proba=0.5,                    #  probability of masking a word\n",
    "    frac_mask_templates=0.1,             # fraction of masking templates (smaller value -> faster, less diverse)\n",
    "    use_proba=True,                      # use words distribution when sampling (if false sample uniform)\n",
    "    top_n=50,                            # consider the fist 50 most likely words\n",
    "    temperature=1.0,                     # higher temperature implies more randomness when sampling\n",
    "    stopwords=['and', 'a', 'but', 'in'], # those words will not be sampled\n",
    "    batch_size_lm=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapesed 0.008645057678222656\n",
      "elapesed 0.00844430923461914\n",
      "elapesed 0.008248567581176758\n",
      "elapesed 0.008549928665161133\n",
      "elapesed 0.008318662643432617\n",
      "elapesed 0.00867152214050293\n",
      "elapesed 0.0084991455078125\n",
      "elapesed 0.008293867111206055\n",
      "elapesed 0.008391857147216797\n",
      "elapesed 0.008346080780029297\n",
      "elapesed 0.008117198944091797\n",
      "elapesed 0.008720159530639648\n",
      "elapesed 0.008159637451171875\n",
      "elapesed 0.008469104766845703\n",
      "elapesed 0.008139610290527344\n",
      "elapesed 0.008698463439941406\n",
      "elapesed 0.008040189743041992\n",
      "elapesed 0.008133411407470703\n",
      "elapesed 0.008237361907958984\n",
      "elapesed 0.008457660675048828\n",
      "elapesed 0.008272647857666016\n",
      "elapesed 0.00999307632446289\n",
      "elapesed 0.013153076171875\n",
      "elapesed 0.010474205017089844\n",
      "elapesed 0.009556770324707031\n",
      "elapesed 0.008692502975463867\n",
      "elapesed 0.008300542831420898\n",
      "elapesed 0.008494853973388672\n",
      "elapesed 0.00845193862915039\n",
      "elapesed 0.008276939392089844\n",
      "elapesed 0.00820469856262207\n",
      "elapesed 0.008226156234741211\n",
      "elapesed 0.008400917053222656\n",
      "elapesed 0.008347511291503906\n",
      "elapesed 0.008304357528686523\n",
      "elapesed 0.008502721786499023\n",
      "elapesed 0.008258819580078125\n",
      "elapesed 0.008335113525390625\n",
      "elapesed 0.008274555206298828\n",
      "elapesed 0.008208036422729492\n",
      "elapesed 0.008148908615112305\n",
      "elapesed 0.008112668991088867\n",
      "elapesed 0.011835575103759766\n",
      "elapesed 0.0110015869140625\n",
      "elapesed 0.008509397506713867\n",
      "elapesed 0.008814573287963867\n",
      "elapesed 0.00833892822265625\n",
      "elapesed 0.008600473403930664\n",
      "elapesed 0.008214950561523438\n",
      "elapesed 0.008388280868530273\n",
      "elapesed 0.008204460144042969\n",
      "elapesed 0.008314847946166992\n",
      "elapesed 0.008118867874145508\n",
      "elapesed 0.008327960968017578\n",
      "elapesed 0.008082389831542969\n",
      "elapesed 0.008183002471923828\n",
      "elapesed 0.008367776870727539\n",
      "elapesed 0.008472919464111328\n",
      "elapesed 0.008329391479492188\n",
      "elapesed 0.008166074752807617\n",
      "elapesed 0.008303403854370117\n",
      "elapesed 0.008190393447875977\n",
      "elapesed 0.008478879928588867\n",
      "elapesed 0.009961128234863281\n",
      "elapesed 0.01080632209777832\n",
      "elapesed 0.009934186935424805\n",
      "elapesed 0.0084991455078125\n",
      "elapesed 0.008395910263061523\n",
      "elapesed 0.008374691009521484\n",
      "elapesed 0.008349180221557617\n",
      "elapesed 0.00813913345336914\n",
      "elapesed 0.008357048034667969\n",
      "elapesed 0.00823974609375\n",
      "elapesed 0.008333683013916016\n",
      "elapesed 0.008183002471923828\n",
      "elapesed 0.008487462997436523\n",
      "elapesed 0.008269309997558594\n",
      "elapesed 0.008322477340698242\n",
      "elapesed 0.00825810432434082\n",
      "elapesed 0.008185625076293945\n",
      "elapesed 0.008238792419433594\n",
      "elapesed 0.008277654647827148\n",
      "elapesed 0.008272409439086914\n",
      "elapesed 0.008470296859741211\n",
      "elapesed 0.009857416152954102\n",
      "elapesed 0.010979890823364258\n",
      "elapesed 0.008935689926147461\n",
      "elapesed 0.008559226989746094\n",
      "elapesed 0.00853729248046875\n",
      "elapesed 0.008139848709106445\n",
      "elapesed 0.0082244873046875\n",
      "elapesed 0.008111715316772461\n",
      "elapesed 0.008196592330932617\n",
      "elapesed 0.008398294448852539\n",
      "elapesed 0.008310556411743164\n",
      "elapesed 0.009046316146850586\n",
      "elapesed 0.008555412292480469\n",
      "elapesed 0.008190155029296875\n",
      "elapesed 0.008394241333007812\n",
      "elapesed 0.00821375846862793\n",
      "elapesed 0.009244918823242188\n",
      "elapesed 0.008574247360229492\n",
      "elapesed 0.008221149444580078\n",
      "elapesed 0.00850367546081543\n",
      "elapesed 0.008446455001831055\n",
      "elapesed 0.008348464965820312\n",
      "elapesed 0.008350133895874023\n",
      "elapesed 0.0084075927734375\n",
      "elapesed 0.008305072784423828\n",
      "elapesed 0.008221626281738281\n",
      "elapesed 0.009376764297485352\n",
      "elapesed 0.008624076843261719\n",
      "elapesed 0.008374214172363281\n",
      "elapesed 0.00864267349243164\n",
      "elapesed 0.008539438247680664\n",
      "elapesed 0.00876927375793457\n",
      "elapesed 0.008473396301269531\n",
      "elapesed 0.008846759796142578\n",
      "elapesed 0.00835275650024414\n",
      "elapesed 0.00835728645324707\n",
      "elapesed 0.009537696838378906\n",
      "elapesed 0.008665084838867188\n",
      "elapesed 0.008097410202026367\n",
      "elapesed 0.00856328010559082\n",
      "elapesed 0.00850224494934082\n",
      "elapesed 0.008684635162353516\n",
      "elapesed 0.00819087028503418\n",
      "elapesed 0.008389711380004883\n",
      "elapesed 0.008352279663085938\n",
      "elapesed 0.008165836334228516\n",
      "elapesed 0.009325742721557617\n",
      "elapesed 0.008600950241088867\n",
      "elapesed 0.00842905044555664\n",
      "elapesed 0.008168697357177734\n",
      "elapesed 0.00830841064453125\n",
      "elapesed 0.008447408676147461\n",
      "elapesed 0.008397579193115234\n",
      "elapesed 0.00839543342590332\n",
      "elapesed 0.008087158203125\n",
      "elapesed 0.008321285247802734\n",
      "elapesed 0.00933384895324707\n",
      "elapesed 0.008643388748168945\n",
      "elapesed 0.009025812149047852\n",
      "elapesed 0.008320093154907227\n",
      "elapesed 0.008644580841064453\n",
      "elapesed 0.008388280868530273\n",
      "elapesed 0.008316755294799805\n",
      "elapesed 0.008415699005126953\n",
      "elapesed 0.008352518081665039\n",
      "elapesed 0.008514404296875\n",
      "elapesed 0.009673595428466797\n",
      "elapesed 0.008788585662841797\n",
      "elapesed 0.008316278457641602\n",
      "elapesed 0.008182287216186523\n",
      "elapesed 0.008391618728637695\n",
      "elapesed 0.00842905044555664\n",
      "elapesed 0.00836634635925293\n",
      "elapesed 0.008414506912231445\n",
      "elapesed 0.0084381103515625\n",
      "elapesed 0.008058786392211914\n",
      "elapesed 0.013979673385620117\n",
      "elapesed 0.012592792510986328\n",
      "elapesed 0.012590169906616211\n",
      "elapesed 0.009938240051269531\n",
      "elapesed 0.01104426383972168\n",
      "elapesed 0.010674238204956055\n",
      "elapesed 0.010882377624511719\n",
      "elapesed 0.009979963302612305\n",
      "elapesed 0.011660337448120117\n",
      "elapesed 0.010943174362182617\n",
      "elapesed 0.012005329132080078\n",
      "elapesed 0.010210275650024414\n",
      "elapesed 0.009530305862426758\n",
      "elapesed 0.009855985641479492\n",
      "elapesed 0.008781909942626953\n",
      "elapesed 0.008866071701049805\n",
      "elapesed 0.008881568908691406\n",
      "elapesed 0.008467435836791992\n",
      "elapesed 0.009044885635375977\n",
      "elapesed 0.009344816207885742\n",
      "elapesed 0.009550333023071289\n",
      "elapesed 0.008783340454101562\n",
      "elapesed 0.008660316467285156\n",
      "elapesed 0.010117053985595703\n",
      "elapesed 0.008284807205200195\n",
      "elapesed 0.008211851119995117\n",
      "elapesed 0.00810098648071289\n",
      "elapesed 0.008178472518920898\n",
      "elapesed 0.00820779800415039\n",
      "elapesed 0.008270740509033203\n",
      "elapesed 0.009389638900756836\n",
      "elapesed 0.008831501007080078\n",
      "elapesed 0.008425235748291016\n",
      "elapesed 0.008398294448852539\n",
      "elapesed 0.008444547653198242\n",
      "elapesed 0.008289575576782227\n",
      "elapesed 0.008204460144042969\n",
      "elapesed 0.008862972259521484\n",
      "elapesed 0.009081363677978516\n",
      "elapesed 0.008549690246582031\n",
      "elapesed 0.010589361190795898\n",
      "elapesed 0.00896143913269043\n",
      "elapesed 0.009636163711547852\n",
      "elapesed 0.008173704147338867\n",
      "elapesed 0.008474111557006836\n",
      "elapesed 0.008410930633544922\n",
      "elapesed 0.00845956802368164\n",
      "elapesed 0.00852823257446289\n",
      "elapesed 0.009216070175170898\n",
      "elapesed 0.009179115295410156\n",
      "elapesed 0.009524822235107422\n",
      "elapesed 0.008778810501098633\n",
      "elapesed 0.009164094924926758\n",
      "elapesed 0.008986473083496094\n",
      "elapesed 0.009389638900756836\n",
      "elapesed 0.00940561294555664\n",
      "elapesed 0.00870966911315918\n",
      "elapesed 0.009468793869018555\n",
      "elapesed 0.008893728256225586\n",
      "elapesed 0.009002923965454102\n",
      "elapesed 0.010762453079223633\n",
      "elapesed 0.012517929077148438\n",
      "elapesed 0.011124610900878906\n",
      "elapesed 0.010039329528808594\n",
      "elapesed 0.011040687561035156\n",
      "elapesed 0.011490583419799805\n",
      "elapesed 0.012082099914550781\n",
      "elapesed 0.011039257049560547\n",
      "elapesed 0.009589433670043945\n",
      "elapesed 0.012226343154907227\n",
      "elapesed 0.009248018264770508\n",
      "elapesed 0.008506298065185547\n",
      "elapesed 0.008564949035644531\n",
      "elapesed 0.008345365524291992\n",
      "elapesed 0.008543014526367188\n",
      "elapesed 0.008471012115478516\n",
      "elapesed 0.008713006973266602\n",
      "elapesed 0.009729146957397461\n",
      "elapesed 0.008626699447631836\n",
      "elapesed 0.008474349975585938\n",
      "elapesed 0.010891437530517578\n",
      "elapesed 0.010532617568969727\n",
      "elapesed 0.010367155075073242\n",
      "elapesed 0.009570837020874023\n",
      "elapesed 0.008466005325317383\n",
      "elapesed 0.008607149124145508\n",
      "elapesed 0.008461236953735352\n",
      "elapesed 0.00831913948059082\n",
      "elapesed 0.00820302963256836\n",
      "elapesed 0.008321285247802734\n",
      "elapesed 0.009347200393676758\n",
      "elapesed 0.008531570434570312\n",
      "elapesed 0.008794784545898438\n",
      "elapesed 0.008437156677246094\n",
      "elapesed 0.008231639862060547\n",
      "elapesed 0.008150577545166016\n",
      "elapesed 0.008281230926513672\n",
      "elapesed 0.008639812469482422\n",
      "elapesed 0.008361339569091797\n",
      "elapesed 0.008322000503540039\n",
      "elapesed 0.010019540786743164\n",
      "elapesed 0.010740041732788086\n",
      "elapesed 0.010411739349365234\n",
      "elapesed 0.009796380996704102\n",
      "elapesed 0.011301517486572266\n",
      "elapesed 0.011533498764038086\n",
      "elapesed 0.010841846466064453\n",
      "elapesed 0.009887456893920898\n",
      "elapesed 0.011803150177001953\n",
      "elapesed 0.01138925552368164\n",
      "elapesed 0.009851932525634766\n",
      "elapesed 0.009485721588134766\n",
      "elapesed 0.009478330612182617\n",
      "elapesed 0.008790254592895508\n",
      "elapesed 0.008406877517700195\n",
      "elapesed 0.008584976196289062\n",
      "elapesed 0.008296489715576172\n",
      "elapesed 0.008242607116699219\n",
      "elapesed 0.008388280868530273\n",
      "elapesed 0.00838470458984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapesed 0.009472131729125977\n",
      "elapesed 0.009256124496459961\n",
      "elapesed 0.008459806442260742\n",
      "elapesed 0.00862264633178711\n",
      "elapesed 0.008530378341674805\n",
      "elapesed 0.009250164031982422\n",
      "elapesed 0.009589672088623047\n",
      "elapesed 0.00845479965209961\n",
      "elapesed 0.009060382843017578\n",
      "elapesed 0.009032487869262695\n",
      "elapesed 0.00902414321899414\n",
      "elapesed 0.008371591567993164\n",
      "elapesed 0.008476018905639648\n",
      "elapesed 0.009579896926879883\n",
      "elapesed 0.00850677490234375\n",
      "elapesed 0.008783102035522461\n",
      "elapesed 0.009502172470092773\n",
      "elapesed 0.009295940399169922\n",
      "elapesed 0.00875997543334961\n",
      "elapesed 0.009137392044067383\n",
      "elapesed 0.012760639190673828\n",
      "elapesed 0.016233444213867188\n",
      "elapesed 0.011783361434936523\n",
      "elapesed 0.012385368347167969\n",
      "elapesed 0.010622024536132812\n",
      "elapesed 0.008990049362182617\n",
      "elapesed 0.00890660285949707\n",
      "elapesed 0.008586406707763672\n",
      "elapesed 0.008546829223632812\n",
      "elapesed 0.008530855178833008\n",
      "elapesed 0.009577035903930664\n",
      "elapesed 0.008770465850830078\n",
      "elapesed 0.008553743362426758\n",
      "elapesed 0.008218526840209961\n",
      "elapesed 0.008252859115600586\n",
      "elapesed 0.008357048034667969\n",
      "elapesed 0.00830078125\n",
      "elapesed 0.008280515670776367\n",
      "elapesed 0.00840902328491211\n",
      "elapesed 0.008298397064208984\n",
      "elapesed 0.009305238723754883\n",
      "elapesed 0.008530616760253906\n",
      "elapesed 0.008333206176757812\n",
      "elapesed 0.008695125579833984\n",
      "elapesed 0.008418083190917969\n",
      "elapesed 0.00834202766418457\n",
      "elapesed 0.008460283279418945\n",
      "elapesed 0.008211374282836914\n",
      "elapesed 0.00870370864868164\n",
      "elapesed 0.008255958557128906\n",
      "elapesed 0.0114288330078125\n",
      "elapesed 0.01117396354675293\n",
      "elapesed 0.011077165603637695\n",
      "elapesed 0.009953975677490234\n",
      "elapesed 0.00946044921875\n",
      "elapesed 0.009183406829833984\n",
      "elapesed 0.008358478546142578\n",
      "elapesed 0.008618354797363281\n",
      "elapesed 0.008661508560180664\n",
      "elapesed 0.008420228958129883\n",
      "elapesed 0.012397527694702148\n",
      "elapesed 0.011174917221069336\n",
      "elapesed 0.01117396354675293\n",
      "elapesed 0.011586189270019531\n",
      "elapesed 0.009264707565307617\n",
      "elapesed 0.013184785842895508\n",
      "elapesed 0.011250019073486328\n",
      "elapesed 0.011638641357421875\n",
      "elapesed 0.009714841842651367\n",
      "elapesed 0.01101541519165039\n",
      "elapesed 0.012402534484863281\n",
      "elapesed 0.011651754379272461\n",
      "elapesed 0.009741067886352539\n",
      "elapesed 0.010555505752563477\n",
      "elapesed 0.011208057403564453\n",
      "elapesed 0.010408639907836914\n",
      "elapesed 0.009271383285522461\n",
      "elapesed 0.011363983154296875\n",
      "elapesed 0.011145591735839844\n",
      "elapesed 0.009440183639526367\n",
      "elapesed 0.009163856506347656\n",
      "elapesed 0.00982666015625\n",
      "elapesed 0.008414983749389648\n",
      "elapesed 0.008481025695800781\n",
      "elapesed 0.008333206176757812\n",
      "elapesed 0.008293867111206055\n",
      "elapesed 0.008438825607299805\n",
      "elapesed 0.008795022964477539\n",
      "elapesed 0.008593559265136719\n",
      "elapesed 0.008511781692504883\n",
      "elapesed 0.01128387451171875\n",
      "elapesed 0.00965571403503418\n",
      "elapesed 0.009888648986816406\n",
      "elapesed 0.011534929275512695\n",
      "elapesed 0.010981321334838867\n",
      "elapesed 0.009460926055908203\n",
      "elapesed 0.011105060577392578\n",
      "elapesed 0.012192487716674805\n",
      "elapesed 0.01237630844116211\n",
      "elapesed 0.01042485237121582\n",
      "elapesed 0.009565591812133789\n",
      "elapesed 0.008543968200683594\n",
      "elapesed 0.009205341339111328\n",
      "elapesed 0.008554220199584961\n",
      "elapesed 0.008839845657348633\n",
      "elapesed 0.009511232376098633\n",
      "elapesed 0.008885860443115234\n",
      "elapesed 0.00935053825378418\n",
      "elapesed 0.009450197219848633\n",
      "elapesed 0.008640527725219727\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "explanation = explainer.explain(text, threshold=0.95, coverage_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: vapid AND flashy AND emotionally AND exercise\n",
      "Precision: 0.98\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "a less flashy but emotionally determined and emotionally vapid exercise in swimming and gymnastics.\n",
      "a fairly flashy but often emotional and emotionally vapid exercise in solitude and meditation.\n",
      "a surprisingly flashy but visually physical and emotionally vapid exercise in biology and conditioning.\n",
      "a much flashy but highly fast and emotionally vapid exercise in comedy and discipline.\n",
      "a surprisingly flashy but surprisingly lively and emotionally vapid exercise in golf and discipline.\n",
      "a decidedly flashy but often emotionally and emotionally vapid exercise in dungeons and yoga.\n",
      "a naturally flashy but sometimes competitive and emotionally vapid exercise in science and safety.\n",
      "a typical flashy but emotionally dramatic and emotionally vapid exercise in cooking and dreams.\n",
      "a rather flashy but socially shy and emotionally vapid exercise in mind and endurance.\n",
      "a fairly flashy but very energetic and emotionally vapid exercise in politics and dreams.\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "a pleasantly flashy but often energetic and emotionally vapid exercise in rhythm and meditation.\n",
      "a relatively flashy but highly enjoyable and emotionally vapid exercise in rhythm and fitness.\n",
      "a visually flashy but fun cynical and emotionally vapid exercise in sports and meditation.\n",
      "a frequently flashy but incredibly entertaining and emotionally vapid exercise in confidence and drama.\n"
     ]
    }
   ],
   "source": [
    "print('Anchor: %s' % (' AND '.join(explanation.anchor)))\n",
    "print('Precision: %.2f' % explanation.precision)\n",
    "print('\\nExamples where anchor applies and model predicts %s:' % pred)\n",
    "print('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_true']]))\n",
    "print('\\nExamples where anchor applies and model predicts %s:' % alternative)\n",
    "print('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_false']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize anchor text explainer with `language_model` sampling (`autoregressive` filling method )\n",
    "\n",
    "* `filling_method='autoregressive'` means that the words are sampled one at the time (autoregressive). Thus, following words to be predicted will be conditioned one the previously generated words.\n",
    "* `frac_mask_templates=1` in this mode (overwriting it with any other value will not be considered).\n",
    "* **This procedure is computationally expensive**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# initialize explainer\n",
    "explainer = AnchorText(\n",
    "    predictor=predict_fn,\n",
    "    sampling_method=\"language_model\",   # use language model to predict the masked words\n",
    "    language_model=language_model,      # language model to be used\n",
    "    filling_method=\"autoregressive\",    # just one pass through the transformer\n",
    "    sample_proba=0.5,                   # probability of masking a word\n",
    "    use_proba=True,                     # use words distribution when sampling (if false sample uniform)\n",
    "    top_n=50,                           # consider the fist 50 most likely words\n",
    "    stopwords=['and', 'a', 'but', 'in'] # those words will not be sampled\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "explanation = explainer.explain(text, threshold=0.95, batch_size=10, coverage_samples=100)\n",
    "\n",
    "print('Anchor: %s' % (' AND '.join(explanation.anchor)))\n",
    "print('Precision: %.2f' % explanation.precision)\n",
    "print('\\nExamples where anchor applies and model predicts %s:' % pred)\n",
    "print('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_true']]))\n",
    "print('\\nExamples where anchor applies and model predicts %s:' % alternative)\n",
    "print('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_false']]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alibi",
   "language": "python",
   "name": "alibi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
