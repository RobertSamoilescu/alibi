{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anchor explanations for movie sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will explain why a certain sentence is classified by a logistic regression as having negative or positive sentiment. The logistic regression is trained on negative and positive movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robert/anaconda3/envs/alibi/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "import string\n",
    "from alibi.explainers import AnchorText\n",
    "from alibi.datasets import fetch_movie_sentiment\n",
    "from alibi.utils.download import spacy_model\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load movie review dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fetch_movie_sentiment` function returns a `Bunch` object containing the features, the targets and the target names for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = fetch_movie_sentiment()\n",
    "movies.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = movies.data\n",
    "labels = movies.target\n",
    "target_names = movies.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define shuffled training, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, train_labels, test_labels = train_test_split(data, labels, test_size=.2, random_state=42)\n",
    "train, val, train_labels, val_labels = train_test_split(train, train_labels, test_size=.1, random_state=42)\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "val_labels = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply CountVectorizer to training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=1)\n",
    "vectorizer.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "clf = LogisticRegression(solver='liblinear')\n",
    "clf.fit(vectorizer.transform(train), train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fn = lambda x: clf.predict(vectorizer.transform(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.9801624284382905\n",
      "Validation accuracy 0.7544910179640718\n",
      "Test accuracy 0.7589841878294202\n"
     ]
    }
   ],
   "source": [
    "preds_train = predict_fn(train)\n",
    "preds_val = predict_fn(val)\n",
    "preds_test = predict_fn(test)\n",
    "print('Train accuracy', accuracy_score(train_labels, preds_train))\n",
    "print('Validation accuracy', accuracy_score(val_labels, preds_val))\n",
    "print('Test accuracy', accuracy_score(test_labels, preds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load spaCy model\n",
    "\n",
    "English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Assigns word vectors, context-specific token vectors, POS tags, dependency parse and named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'en_core_web_md'\n",
    "spacy_model(model=model)\n",
    "nlp = spacy.load(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize anchor text explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = AnchorText(nlp=nlp, predictor=predict_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = movies.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification .\n"
     ]
    }
   ],
   "source": [
    "text = data[4]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: negative\n"
     ]
    }
   ],
   "source": [
    "pred = class_names[predict_fn([text])[0]]\n",
    "alternative =  class_names[1 - predict_fn([text])[0]]\n",
    "print('Prediction: %s' % pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "explanation = explainer.explain(text, threshold=0.95, sampling_method='unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use_unk=True means we will perturb examples by replacing words with UNKs. Let us now take a look at the anchor. The word 'exercise' basically guarantees a negative prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: flashy\n",
      "Precision: 0.99\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "a UNK flashy UNK UNK opaque and emotionally vapid exercise in style UNK mystification .\n",
      "a UNK flashy UNK UNK UNK and emotionally UNK exercise UNK UNK and UNK UNK\n",
      "a UNK flashy UNK narratively opaque UNK UNK UNK exercise in style and UNK UNK\n",
      "UNK visually flashy UNK narratively UNK and emotionally UNK UNK UNK UNK UNK mystification .\n",
      "UNK UNK flashy UNK UNK opaque and emotionally UNK UNK in UNK and UNK .\n",
      "a visually flashy but UNK UNK and UNK UNK UNK in style UNK mystification .\n",
      "a visually flashy but UNK opaque UNK emotionally vapid UNK in UNK and mystification .\n",
      "a UNK flashy but narratively UNK UNK emotionally vapid exercise in style UNK mystification UNK\n",
      "a UNK flashy but narratively opaque UNK emotionally vapid exercise in style and mystification .\n",
      "a visually flashy UNK UNK opaque UNK UNK UNK exercise in UNK UNK UNK .\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "UNK UNK flashy but narratively UNK and UNK UNK UNK in style and UNK UNK\n"
     ]
    }
   ],
   "source": [
    "print('Anchor: %s' % (' AND '.join(explanation.anchor)))\n",
    "print('Precision: %.2f' % explanation.precision)\n",
    "print('\\nExamples where anchor applies and model predicts %s:' % pred)\n",
    "print('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_true']]))\n",
    "print('\\nExamples where anchor applies and model predicts %s:' % alternative)\n",
    "print('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_false']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the perturbation distribution\n",
    "Let's try this with another perturbation distribution, namely one that replaces words by similar words instead of UNKs.\n",
    "\n",
    "Explanation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "explanation = explainer.explain(text, threshold=0.95, sampling_method=\"unknown\", sample_proba=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The anchor now shows that we need more to guarantee the negative prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: flashy\n",
      "Precision: 0.99\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "a UNK flashy UNK UNK opaque and emotionally vapid exercise in style UNK mystification .\n",
      "a UNK flashy UNK UNK UNK and emotionally UNK exercise UNK UNK and UNK UNK\n",
      "a UNK flashy UNK narratively opaque UNK UNK UNK exercise in style and UNK UNK\n",
      "UNK visually flashy UNK narratively UNK and emotionally UNK UNK UNK UNK UNK mystification .\n",
      "UNK UNK flashy UNK UNK opaque and emotionally UNK UNK in UNK and UNK .\n",
      "a visually flashy but UNK UNK and UNK UNK UNK in style UNK mystification .\n",
      "a visually flashy but UNK opaque UNK emotionally vapid UNK in UNK and mystification .\n",
      "a UNK flashy but narratively UNK UNK emotionally vapid exercise in style UNK mystification UNK\n",
      "a UNK flashy but narratively opaque UNK emotionally vapid exercise in style and mystification .\n",
      "a visually flashy UNK UNK opaque UNK UNK UNK exercise in UNK UNK UNK .\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "UNK UNK flashy but narratively UNK and UNK UNK UNK in style and UNK UNK\n"
     ]
    }
   ],
   "source": [
    "print('Anchor: %s' % (' AND '.join(explanation.anchor)))\n",
    "print('Precision: %.2f' % explanation.precision)\n",
    "print('\\nExamples where anchor applies and model predicts %s:' % pred)\n",
    "print('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_true']]))\n",
    "print('\\nExamples where anchor applies and model predicts %s:' % alternative)\n",
    "print('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_false']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the token perturbation distribution sample words that are more similar to the ground truth word via the `top_n` argument. Smaller values (default=100) should result in sentences that are more coherent and thus more in the distribution of natural language which could influence the returned anchor. By setting the `use_probability_proba` to True, the sampling distribution for perturbed tokens is proportional to the similarity score between the possible perturbations and the original word. We can also put more weight on similar words via the `temperature` argument. Lower values of `temperature` increase the sampling weight of more similar words. The following example will perturb tokens in the original sentence with probability equal to `sample_proba`. The sampling distribution for the perturbed tokens is proportional to the similarity score between the ground truth word and each of the `top_n` words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchor: emotionally\n",
      "Precision: 0.97\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "an aesthetically snazzy but narratively opaque and emotionally incomprehensible isometric in style and selfishness .\n",
      "another visually gaudy but narratively translucent and emotionally litany exercise near style and mystification .\n",
      "a graphically flashy but stylistically opaque and emotionally vapid gym arround style and mystification .\n",
      "every physically flashy but thematically opaque and emotionally incomprehensible exercise inside style and mystification .\n",
      "a graphically flashy but stylistically opaque and emotionally vapid exercise in style and mystification .\n",
      "another aesthetically snazzy but artistically opaque and emotionally vapid excercise in style and delusion .\n",
      "a graphically unwieldy but narratively opaque and emotionally litany exercise in style and mystification .\n",
      "another visually snazzy but narratively translucent and emotionally vapid isometric of dress and materialism .\n",
      "a visually gimmicky but narratively transparent and emotionally vapid contraction in minimalism and mystification .\n",
      "a visually flashy but artistically translucent and emotionally vapid exercise within minimalism and mystification .\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "an graphically flashy but narratively solid and emotionally inane excercise in style and paranoia .\n",
      "a subtly snazzy but artistically opaque and emotionally asinine bodyweight in style and mirage .\n",
      "a visually snazzy but narratively opaque and emotionally vapid excercise within style and materialism .\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "explanation = explainer.explain(text, threshold=0.95, sampling_method='similarity', sample_proba=0.5,\n",
    "                                use_unk=False, top_n=20, temperature=.2)\n",
    "\n",
    "print('Anchor: %s' % (' AND '.join(explanation.anchor)))\n",
    "print('Precision: %.2f' % explanation.precision)\n",
    "print('\\nExamples where anchor applies and model predicts %s:' % pred)\n",
    "print('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_true']]))\n",
    "print('\\nExamples where anchor applies and model predicts %s:' % alternative)\n",
    "print('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_false']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi.utils.lang_model import DistilbertBaseUncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "model = DistilbertBaseUncased()\n",
    "\n",
    "# initialize explainer\n",
    "explainer = AnchorText(language_model=model, predictor=predict_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"it's so laddish and juvenile , only teenage boys could possibly find it funny .\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = data[1]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's [MASK] laddish and [MASK], only [MASK] boys [MASK] [MASK] find [MASK] [MASK].\n",
      "[MASK]'[MASK] so [MASK] and [MASK], only [MASK] boys [MASK] [MASK] find it [MASK].\n",
      "it's so laddish and [MASK], only teenage boys [MASK] possibly find [MASK] funny.\n",
      "[MASK]'s [MASK] laddish and juvenile, [MASK] teenage [MASK] [MASK] [MASK] [MASK] [MASK] funny.\n",
      "[MASK]'s [MASK] [MASK] and juvenile, only [MASK] [MASK] could [MASK] find it [MASK].\n",
      "[MASK]'[MASK] [MASK] laddish and juvenile, only teenage boys could possibly [MASK] [MASK] [MASK].\n",
      "it'[MASK] so [MASK] and juvenile, only teenage [MASK] [MASK] [MASK] [MASK] [MASK] funny.\n",
      "it'[MASK] so [MASK] and [MASK], [MASK] [MASK] boys could possibly [MASK] [MASK] funny.\n",
      "[MASK]'[MASK] [MASK] [MASK] and juvenile, [MASK] teenage [MASK] [MASK] [MASK] find [MASK] funny.\n",
      "it'[MASK] [MASK] laddish and [MASK], [MASK] [MASK] boys [MASK] possibly [MASK] [MASK] funny.\n",
      "it'[MASK] so [MASK] and [MASK], only [MASK] [MASK] could [MASK] [MASK] it funny.\n",
      "it'[MASK] so [MASK] and [MASK], [MASK] [MASK] boys [MASK] possibly [MASK] it [MASK].\n",
      "[MASK]'s so laddish and juvenile, [MASK] [MASK] [MASK] could [MASK] [MASK] [MASK] funny.\n",
      "it'[MASK] so laddish and [MASK], [MASK] [MASK] boys [MASK] possibly find it [MASK].\n",
      "it's so [MASK] and juvenile, [MASK] [MASK] [MASK] could possibly [MASK] [MASK] funny.\n",
      "it'[MASK] so laddish and juvenile, [MASK] [MASK] boys could possibly [MASK] [MASK] [MASK].\n",
      "[MASK]'s [MASK] laddish and juvenile, only teenage boys could [MASK] find [MASK] [MASK].\n",
      "it'[MASK] [MASK] laddish and [MASK], [MASK] teenage [MASK] [MASK] possibly [MASK] [MASK] funny.\n",
      "it'[MASK] so [MASK] and juvenile, only [MASK] boys could possibly [MASK] [MASK] funny.\n",
      "[MASK]'s [MASK] laddish and juvenile, only [MASK] boys could possibly [MASK] [MASK] funny.\n",
      "it's so laddish and juvenile, only [MASK] boys [MASK] [MASK] [MASK] [MASK] [MASK].\n",
      "it's so laddish and [MASK], only teenage [MASK] could possibly find [MASK] [MASK].\n",
      "it'[MASK] so laddish and juvenile, only [MASK] boys [MASK] possibly [MASK] it [MASK].\n",
      "it's so laddish and juvenile, [MASK] teenage [MASK] [MASK] possibly find it funny.\n",
      "it'[MASK] [MASK] [MASK] and juvenile, [MASK] teenage boys could [MASK] [MASK] it [MASK].\n",
      "[MASK]'s [MASK] [MASK] and [MASK], only teenage [MASK] could [MASK] find [MASK] [MASK].\n",
      "[MASK]'s so [MASK] and [MASK], only [MASK] [MASK] [MASK] possibly find [MASK] [MASK].\n",
      "it's so [MASK] and [MASK], only teenage [MASK] [MASK] possibly [MASK] it [MASK].\n",
      "it'[MASK] [MASK] [MASK] and [MASK], only teenage boys could possibly [MASK] it funny.\n",
      "it's so laddish and [MASK], only [MASK] [MASK] [MASK] possibly [MASK] it funny.\n",
      "it's so [MASK] and juvenile, only [MASK] [MASK] [MASK] [MASK] find it [MASK].\n",
      "[MASK]'[MASK] so laddish and [MASK], [MASK] teenage boys [MASK] possibly find it funny.\n",
      "[MASK]'[MASK] so [MASK] and juvenile, only teenage [MASK] [MASK] possibly [MASK] [MASK] [MASK].\n",
      "it's so laddish and juvenile, only teenage boys [MASK] possibly [MASK] it [MASK].\n",
      "it's so laddish and juvenile, [MASK] [MASK] boys could possibly find it [MASK].\n",
      "Anchor: possibly AND so AND laddish AND juvenile\n",
      "Precision: 0.97\n",
      "\n",
      "Examples where anchor applies and model predicts negative:\n",
      "it's so laddish and juvenile, though those boys could possibly find it funny.\n",
      "it's so laddish and juvenile, even teenage boys could possibly find it entertaining.\n",
      "it's so laddish and juvenile, even normal boys could possibly find it annoying.\n",
      "it's so laddish and juvenile, although teenage boys could possibly find it helpful.\n",
      "it's so laddish and juvenile, only young boys could possibly find it amusing.\n",
      "it's so laddish and juvenile, but teenage boys could possibly find it frightening.\n",
      "it's so laddish and juvenile, so teenage boys could possibly find it better.\n",
      "it's so laddish and juvenile, but old boys could possibly find it amusing.\n",
      "it's so laddish and juvenile, so adolescent boys could possibly find it funny.\n",
      "it's so laddish and juvenile, some young boys could possibly find it boring.\n",
      "\n",
      "Examples where anchor applies and model predicts positive:\n",
      "it's so laddish and juvenile, but you kids cannot possibly find it funny.\n",
      "it's so laddish and juvenile, teenage teenage girls could possibly make it funny.\n",
      "it's so laddish and juvenile, some teenage girls could possibly make it funny.\n",
      "it's so laddish and juvenile, though teenage kids could possibly call it funny.\n",
      "it's so laddish and juvenile, but teenage boys could possibly make it funny.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "explanation = explainer.explain(\n",
    "    text, \n",
    "    threshold=0.95, \n",
    "    sampling_method=\"language_model\",\n",
    "    filling_method=\"parallel\",\n",
    "    sample_proba=0.5,\n",
    "    stopwords=['It', 'in', 'the', 'a', 'and'],\n",
    "    punctuation=string.punctuation,\n",
    "    top_n=50,\n",
    "    prec_mask_templates=0.1,\n",
    ")\n",
    "\n",
    "print('Anchor: %s' % (' AND '.join(explanation.anchor)))\n",
    "print('Precision: %.2f' % explanation.precision)\n",
    "print('\\nExamples where anchor applies and model predicts %s:' % pred)\n",
    "print('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_true']]))\n",
    "print('\\nExamples where anchor applies and model predicts %s:' % alternative)\n",
    "print('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_false']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(0)\n",
    "# text = \"This is a good book.\"\n",
    "\n",
    "# explanation = explainer.explain(\n",
    "#     text, \n",
    "#     threshold=0.95, \n",
    "#     sampling_method=\"language_model\",\n",
    "#     filling_method=\"autoregressive\",\n",
    "#     sample_proba=1.0,\n",
    "#     stopwords=['in', 'the', 'a', 'and'],\n",
    "#     k=50,\n",
    "# )\n",
    "\n",
    "# print('Anchor: %s' % (' AND '.join(explanation.anchor)))\n",
    "# print('Precision: %.2f' % explanation.precision)\n",
    "# print('\\nExamples where anchor applies and model predicts %s:' % pred)\n",
    "# print('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_true']]))\n",
    "# print('\\nExamples where anchor applies and model predicts %s:' % alternative)\n",
    "# print('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_false']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alibi",
   "language": "python",
   "name": "alibi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
